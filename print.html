<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Supercomputing for Big Data - Lab Manual</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction/index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="introduction/goal-of-this-lab.html"><strong aria-hidden="true">1.1.</strong> Goal of this lab</a></li><li class="chapter-item expanded "><a href="introduction/communication-with-tas.html"><strong aria-hidden="true">1.2.</strong> Communication with TAs</a></li><li class="chapter-item expanded "><a href="introduction/code-repositories.html"><strong aria-hidden="true">1.3.</strong> Code repositories</a></li><li class="chapter-item expanded "><a href="introduction/groups.html"><strong aria-hidden="true">1.4.</strong> Groups</a></li><li class="chapter-item expanded "><a href="introduction/aws.html"><strong aria-hidden="true">1.5.</strong> AWS</a></li><li class="chapter-item expanded "><a href="introduction/grading.html"><strong aria-hidden="true">1.6.</strong> Grading</a></li></ol></li><li class="chapter-item expanded "><a href="getting-started/index.html"><strong aria-hidden="true">2.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="getting-started/docker.html"><strong aria-hidden="true">2.1.</strong> Docker</a></li><li class="chapter-item expanded "><a href="getting-started/scala.html"><strong aria-hidden="true">2.2.</strong> Scala</a></li><li class="chapter-item expanded "><a href="getting-started/apache-spark/index.html"><strong aria-hidden="true">2.3.</strong> Apache Spark</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="getting-started/apache-spark/resilient-distributed-datasets.html"><strong aria-hidden="true">2.3.1.</strong> Resilient Distributed Datasets</a></li><li class="chapter-item expanded "><a href="getting-started/apache-spark/dataframe-and-dataset.html"><strong aria-hidden="true">2.3.2.</strong> Dataframe and Dataset</a></li><li class="chapter-item expanded "><a href="getting-started/apache-spark/packaging-your-application-using-sbt.html"><strong aria-hidden="true">2.3.3.</strong> Packaging your application using SBT</a></li></ol></li><li class="chapter-item expanded "><a href="getting-started/amazon-web-services.html"><strong aria-hidden="true">2.4.</strong> Amazon Web Services</a></li><li class="chapter-item expanded "><a href="getting-started/apache-kafka.html"><strong aria-hidden="true">2.5.</strong> Apache Kafka</a></li><li class="chapter-item expanded "><a href="getting-started/openstreetmap.html"><strong aria-hidden="true">2.6.</strong> OpenStreetMap</a></li></ol></li><li class="chapter-item expanded "><a href="lab1/index.html"><strong aria-hidden="true">3.</strong> Lab 1</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab1/before-you-start.html"><strong aria-hidden="true">3.1.</strong> Before you start</a></li><li class="chapter-item expanded "><a href="lab1/assignment.html"><strong aria-hidden="true">3.2.</strong> Assignment</a></li><li class="chapter-item expanded "><a href="lab1/deliverables.html"><strong aria-hidden="true">3.3.</strong> Deliverables</a></li><li class="chapter-item expanded "><a href="lab1/rubric.html"><strong aria-hidden="true">3.4.</strong> Rubric</a></li></ol></li><li class="chapter-item expanded "><a href="lab2/index.html"><strong aria-hidden="true">4.</strong> Lab 2</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab2/before-you-start.html"><strong aria-hidden="true">4.1.</strong> Before you start</a></li><li class="chapter-item expanded "><a href="lab2/assignment.html"><strong aria-hidden="true">4.2.</strong> Assignment</a></li><li class="chapter-item expanded "><a href="lab2/deliverables.html"><strong aria-hidden="true">4.3.</strong> Deliverables</a></li><li class="chapter-item expanded "><a href="lab2/rubric.html"><strong aria-hidden="true">4.4.</strong> Rubric</a></li></ol></li><li class="chapter-item expanded "><a href="lab3/index.html"><strong aria-hidden="true">5.</strong> Lab 3</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab3/before-you-start.html"><strong aria-hidden="true">5.1.</strong> Before you start</a></li><li class="chapter-item expanded "><a href="lab3/assignment.html"><strong aria-hidden="true">5.2.</strong> Assignment</a></li><li class="chapter-item expanded "><a href="lab3/deliverables.html"><strong aria-hidden="true">5.3.</strong> Deliverables</a></li><li class="chapter-item expanded "><a href="lab3/rubric.html"><strong aria-hidden="true">5.4.</strong> Rubric</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="faq.html"><strong aria-hidden="true">6.</strong> FAQ</a></li><li class="chapter-item expanded "><a href="quiz_example.html"><strong aria-hidden="true">7.</strong> Quiz example</a></li><li class="chapter-item expanded "><a href="links.html"><strong aria-hidden="true">8.</strong> Useful links</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Supercomputing for Big Data - Lab Manual</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>What requirement would you place on a city or town to live in? As far as the
teaching assistants (TAs) are concerned, that have prepared this lab, it is
paramount there are <em>plenty of beer breweries</em> nearby, such that we may quench
our thirst after a hard day's work at the Supercomputing for Big Data lab!</p>
<p>It turns out that we can achieve all the learning objectives of the course by
attempting to answer this question and some variants on it. To this end, we
limit ourselves to a context where we can only use the <a href="https://www.openstreetmap.org">OpenStreetMap</a> dataset,
together with various industry-standard frameworks and tools, such as the <a href="https://www.scala-lang.org/">Scala
programming language</a> to write our programs, <a href="https://spark.apache.org">Apache Spark</a> to process batches
of data, <a href="https://kafka.apache.org">Apache Kafka</a> to process streaming data, and the <a href="https://aws.amazon.com">Amazon Web Services</a>
(AWS) to scale out our solutions in terms of performance (but also cost!).
In this context, we will face several challenges that are similar to what you
could find in the industry - and we will learn to deal with them efficiently.</p>
<h4><a class="header" href="#using-this-manual" id="using-this-manual">Using this manual</a></h4>
<p>You can browse through the manual by using the table of contents on the left. To
go to the next page, you can also click on the <code>&gt;</code>'s on the right of this page.
This manual is generated by <a href="https://github.com/rust-lang/mdBook">mdBook</a>, and the sources can be found on <a href="https://github.com/abs-tudelft/sbd">GitHub</a>.
Feel free to kindly report issues and/or make pull requests to suggest or
implement improvements! If there are any major changes, we will notify everyone
on Brightspace and Discord.</p>
<h2><a class="header" href="#goal-of-this-lab" id="goal-of-this-lab">Goal of this Lab</a></h2>
<p>The goal of this lab is to achieve the Course Learning Objectives, that we 
repeat here.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<p>We will achieve these learning objectives by performing the following tasks,
spread out over three labs.</p>
<ul>
<li>You will work with Apache Spark, the MapReduce programming paradigm, and the 
Scala programming language, which is widely used in this domain.</li>
<li>You will approach a big data problem analytically and practically.</li>
<li>You will work with cloud-based systems.</li>
<li>You will deal with existing infrastructures for big data.</li>
<li>You will modify an existing application to operate in a streaming data 
context.</li>
</ul>
<h3><a class="header" href="#communication-with-tas" id="communication-with-tas">Communication with TAs</a></h3>
<h4><a class="header" href="#tldr" id="tldr">TL;DR:</a></h4>
<h5><a class="header" href="#text" id="text">Text:</a></h5>
<ul>
<li><code>#general</code>: for general questions that may be useful for other students.</li>
<li><code>#queue</code>: for joining the queue for voice attention from TAs (<code>q!join</code>)</li>
<li><code>#lab-clarifications</code>: Clarifications for common problems.</li>
<li><code>#manual-notifications</code>: Changes in the lab manual.</li>
</ul>
<h5><a class="header" href="#voice" id="voice">Voice:</a></h5>
<ul>
<li><code>ta-channel-x</code>: Voice channels for TAs. Join within 30 secs when <code>q!remove</code>d in <code>#queue</code>.</li>
<li><code>General</code>: Used in dire need only.</li>
</ul>
<h4><a class="header" href="#lab-on-discord" id="lab-on-discord">Lab on Discord</a></h4>
<p>Direct communication with TAs during official lab hours will take place on
<a href="https://discord.com/">Discord</a>, until we get rid of the coronavirus. The Discord invitation link will
be on Brightspace. Outside official lab hours, they will still hang around
Discord, and <em>may or may not</em> have time to answer you directly. </p>
<p>Voice attention from the TAs can be requested through a Discord queueing bot.
Enter the queue by typing <code>q!join</code> in the <code>#queue</code> chat channel. Once it is
your turn, a TA will remove you from the queue by using <code>q!remove @&lt;username&gt;</code>.
You will then get a notification from Discord that your username was mentioned.
You can then join the TA voice channel of the TA that removed you from the
queue. If you do not join the voice channel within 30 seconds, the TA will move
to the next person in the queue, and you will have to join at the back again,
sorry!</p>
<p>If and only if you think other students may benefit from the answer to a textual
question (perhaps something is unclear in the lab manual), please ask your
question in the <code>#general</code> chat, so all students can observe our reply.
Just to be clear: this does not include advice on your specific approach.
Also, do not copy and paste large portions of code in the chat.</p>
<p>There is a <code>#lab-clarifications</code> channel that is read-only for students, where
we may post some clarifications to common problems. In general, these will also
be updated in this manual. Any activity on the source repository of the manual
will be notified on <code>#manual-notifications</code>.</p>
<p>To prevent multiple TAs solving problems for your at the same time, contact only
one TA with your request or problem. Please do not broadcast the message to all
TAs. Only send your request to one TA.</p>
<p>If you already use Discord for gaming, please consider changing your nickname on
the server (e.g. _xX_ulTrA1337fRaGmOnST3r_Xx) to something more recognizable
as you (e.g. J. Doe or Jane D. or Jane Doe). This change is local to the lab
server, so will not appear on other servers.</p>
<h3><a class="header" href="#code-repositories" id="code-repositories">Code repositories</a></h3>
<p>For this lab, you will write a lot of code. All (incremental additions to) the
code for the lab assignments are kept in a private <a href="https://git-scm.com">git</a> repository that is
hosted on <a href="https://github.com">GitHub</a>, inside the <a href="https://classroom.github.com">GitHub Classroom</a> used for this course.
(Please don't register yet until you've read the full introduction.) This is the
<strong>only way</strong> to turn in code, any other channels, such as e-mail, are rejected.</p>
<p>If you do not know how to use git, you <em>must</em> familiarize yourself with it
first. This is an essential skill for your future career. We consider this a
prerequisite to the course. There are many tutorials online, and sometimes
<a href="https://rogerdudler.github.io/git-guide">cheatsheets</a> come in handy when you start to use git. There are also <a href="https://www.gitkraken.com">GUIs</a>
that may be useful to those that don't like working on the command-line.</p>
<p>The snapshot of the code in your assignment repository on the <code>master</code> branch,
at the moment the deadline expires, will be your submission. Make sure to make
your last commit before the deadline!</p>
<p>We recommend students to work on branches, and make pull requests to their
<code>master</code> branch once the code they are trying to implement is complete. These
pull requests can be reviewed by their teammates or by the TAs to provide early
feedback. </p>
<p>The feedback mechanism is also explicitly available in your GitHut Classroom
repository as a pull request that is made when your repository is first
initialized. To trigger the TA's to start reviewing your master branch, you can
also use this pull request (PRs). It will be named <code>Feedback #1</code> and can be
found by clicking <code>Pull requests</code>, then the <code>Feedback #1</code> PR. You can then ask
for a review from the TAs; either user <code>mbrobbel</code> or <code>johanpel</code>. Also see the
figure below:</p>
<p><img src="introduction/../assets/images/review_request.png" alt="Where to find the pull requests and ask for review" /></p>
<p>Here is an example of feedback that we can provide based on the <code>Feedback #1</code> PR.
Note that you can also request reviews on any other PR if you want.</p>
<p><img src="introduction/../assets/images/review.png" alt="Example of a review requested through GitHub Classroom" /></p>
<p>Because we have limited capacity, we do ask you to request a review only when
you've made either very significant contributions, or preferably when you think
you have completed the assignment. TA priority will be based on groups ordered
by number of review requests performed (e.g. groups who have had only one
request will go before groups who have had several reviews).</p>
<h2><a class="header" href="#groups" id="groups">Groups</a></h2>
<p>You may work alone or in groups of at most two students. In the case of groups,
it is recommended that both students attempt to solve the problem before
converging to a single implementation.</p>
<p>In any case, even if you work alone, you must register your group in both GitHub
Classroom and Brightspace. We know this is inconvenient, but there is no
integration between Classroom and Brightspace yet. We use Brightspace for grade
administration, and GitHub Classroom for the code.</p>
<p>All groups in Classroom <strong>must</strong> be named after their respective name in
Brightspace.</p>
<h3><a class="header" href="#aws" id="aws">AWS</a></h3>
<p>Lab 2 requires a significant amount of computations performed on a lot of data
so we need a large compute and storage infrastructure to run the program. In
this lab, we will use AWS to facilitate this.</p>
<p>As a student you are eligible for credits on this platform. We would like you to
register for the <a href="https://education.github.com/pack">GitHub Student Developer Pack</a>, as soon as you decide to take
this course. This gives you access to around 100 dollars worth of credits. This
should be ample to complete lab 2. Don't forget to register on AWS using the
referral link from GitHub. Use the &quot;AWS Account ID&quot; option (requiring a credit
card), if you can.</p>
<p><strong>Make sure you register for these credits as soon as possible! You can always
ask the TAs if you run into any trouble.</strong></p>
<h2><a class="header" href="#early-feedback-and-grading" id="early-feedback-and-grading">Early Feedback and Grading</a></h2>
<p>Grading depends on the specific lab exercise and is explained in the associated
parts of the guide, but is generally based on:</p>
<ol>
<li>Your code</li>
<li>Your report</li>
</ol>
<p>Additionally, there is a potential oral exam based on the grade of a quiz at 
the end of the course (more about this at the end of this section).</p>
<p>All your code must be incrementally updated on GitHub Classroom. The reports are
also stored in your repository as the <code>README.md</code>. At some point, the deadline
will expire, and you will not be able to update your code and report. As 
explained previously, the code on the <code>master</code> branch will be graded.</p>
<p>We strive for a course where it is clear to you how you are doing before handing
in your project for grading, as well as making it clear how you are graded. In
this way, we do not have to rely on resits in our quest to achieve the learning
objectives, but will provide feedback much earlier, before handing in your
assignment for grading. Thus, this lab knows no resit (this is also only
required by the examination rules for written exams).</p>
<p>There are two mechanisms by which you can obtain <a href="https://en.wikipedia.org/wiki/Formative_assessment">early feedback</a> about your 
work.</p>
<p>a. Each lab will have an associated <a href="https://en.wikipedia.org/wiki/Rubric_(academic)">Rubric</a>, which is like a table with
indicators for specific grades in specific parts of the assignment, that are 
related to the learning objectives of this course.</p>
<p>b. You can request reviews from the TAs in GitHub Classroom on e.g. your pull
requests with incremental changes to your code. This includes requesting 
reviews on your <code>README.md</code> (i.e. your report).</p>
<p>Therefore, by spending the required amount of time to learn, using the Rubric to
reflect on your own work, and by making use of the review functionality of
GitHub Classroom, it is incredibly unlikely that you will fail this course. At
the same time, do remember that programming experience is a prerequisite for
this course. We unfortunately do not have time to teach you how to program.</p>
<p>Regarding the oral exam, there is a possibility that some group members do not
contribute to achieving the lab goals, in turn causing them to not achieve the
learning objectives (sometimes called freeloaders). Because the TU Delft strives
to deliver Masters of Science of the highest quality, we must ensure that all
students that pass this course have achieved the learning objectives. To this
end, in one of the final weeks of the course, a multiple-choice quiz will take
place that is incredibly easy to pass if you achieved all learning goals through
the lab. If you do not pass (there will be a specific threshold), you are
invited to an oral examination where we will discuss the implementation of the
code. Based on this, we will determine a grade multiplier between zero and one.
Thus, make sure you understand every detail of the code your group may produce.
The quiz will be performed on Brightspace during a lecture slot. An example of
quiz questions is found <a href="introduction/../quiz_example.html">here</a>.</p>
<h3><a class="header" href="#regarding-plagiarism" id="regarding-plagiarism">Regarding plagiarism</a></h3>
<p>All rules of the TU Delft and master program regarding plagiarism apply. It's
fine to copy little pieces of code that you understand from e.g. StackOverflow,
to build up your own desired functionality around it. It is definitely <strong>not</strong>
allowed to copy portions or the whole solution from other students. If you did
that, you wouldn't be achieving the learning objectives!</p>
<p>Additionally, <strong>DO NOT</strong> make your code publicly available by e.g. forking or
copying your repository publicly. The examination rules of the Delft University
of Technology explain that even though perhaps you wrote the code yourself,
making it publicly available makes you a potential accomplice to plagiarism and
you may receive the same punishment as whoever copied it from you. We repeat:
<em>DO NOT make your code publicly available</em>.</p>
<h1><a class="header" href="#getting-started" id="getting-started">Getting Started</a></h1>
<p>In this chapter, we will cover some of the concepts and technologies that are
used during the course.</p>
<h2><a class="header" href="#example-repository" id="example-repository">Example repository</a></h2>
<p>The examples in this chapter are accompanied by some code. You can download this
code from its <a href="https://github.com/abs-tudelft/sbd-example">online repository</a>. Unless stated otherwise, we usually run
commands in the root folder of this repository. To get this code and go into the
root folder, you could run:</p>
<pre><code class="language-bash">git clone https://github.com/abs-tudelft/sbd-example.git
cd sbd-example
</code></pre>
<p>For command-line commands, we're going to assume we're using Linux with Bash. If
you're on Windows or Mac, you have to figure out how to do stuff yourself, or
perhaps use a virtual machine or container.</p>
<p>This chapter will continue to introduce the following topics:</p>
<h2><a class="header" href="#docker" id="docker">Docker</a></h2>
<p>An application that allows the user to package and run software (like Spark and
Kafka and the programs we write for them) in an isolated environment: a
container.</p>
<h2><a class="header" href="#scala" id="scala">Scala</a></h2>
<p>A programming language that runs on the Java Virtual Machine (JVM). This is our
(mandatory!) language of choice during the lab assignments. We will use it to
program for both Apache Spark and Apache Kafka.</p>
<h2><a class="header" href="#apache-spark" id="apache-spark">Apache Spark</a></h2>
<p>A framework for processing large amounts of data on multiple machines in a
robust way. We will build our application for labs 1 and 2 using Spark.</p>
<h2><a class="header" href="#amazon-web-services" id="amazon-web-services">Amazon Web Services</a></h2>
<p>AWS, which provide theoretically unlimited compute infrastructure, allowing us
to process a large dataset in lab 2.</p>
<h2><a class="header" href="#apache-kafka" id="apache-kafka">Apache Kafka</a></h2>
<p>A framework for building so-called data pipelines, in which potentially many
producers and consumers process real-time, streaming data. In lab 3, we will
take the application from labs 1 and 2 and modify it to process data in
real-time, using Kafka.</p>
<h2><a class="header" href="#openstreetmap" id="openstreetmap">OpenStreetMap</a></h2>
<p>A open source project capturing geographic data from all over the world. The
assignments of this lab are based on (parts of) this data set.</p>
<h2><a class="header" href="#docker-1" id="docker-1">Docker</a></h2>
<p>According to the <a href="https://docs.docker.com/get-started">Docker Documentation</a></p>
<blockquote>
<p>Docker is a platform for developers and sysadmins to develop, deploy, and run
applications with containers. The use of Linux containers to deploy
applications is called containerization. Containers are not new, but their
use for easily deploying applications is. Containerization is increasingly
popular because containers are:</p>
<p>Flexible</p>
<ul>
<li>Even the most complex applications can be containerized.</li>
</ul>
<p>Lightweight</p>
<ul>
<li>Containers leverage and share the host kernel.</li>
</ul>
<p>Interchangeable</p>
<ul>
<li>You can deploy updates and upgrades on-the-fly.</li>
</ul>
<p>Portable</p>
<ul>
<li>You can build locally, deploy to the cloud, and run anywhere.</li>
</ul>
<p>Scalable</p>
<ul>
<li>You can increase and automatically distribute container replicas.</li>
</ul>
<p>Stackable</p>
<ul>
<li>You can stack services vertically and on-the-fly.</li>
</ul>
</blockquote>
<p>For this course, we use Docker primarily to ensure every student is using the
exact same platform for their applications, and to avoid certain
platform-specific issues and peculiarities.</p>
<blockquote>
<p>You are <strong>not</strong> required to use Docker for this lab when you feel comfortable
setting up the required tools on your own system.</p>
</blockquote>
<p>A basic understanding of some <a href="https://docs.docker.com/">Docker</a> concepts helps
in getting started with this course. <a href="https://docs.docker.com/get-started/">Part 1: Orientation and
setup</a> of the <a href="https://docs.docker.com/get-started/">Get Started
Guide</a> covers the basic
<a href="https://docs.docker.com/">Docker</a> concepts used in this course.</p>
<p>Before trying the lab assignments and tutorials in the next sections, make sure
you <a href="https://docs.docker.com/install/#supported-platforms">Install Docker
(stable)</a> and test your
installation by running the simple <a href="https://hub.docker.com/_/hello-world">Hello World
image</a>.</p>
<pre><code class="language-bash">docker run hello-world
</code></pre>
<h3><a class="header" href="#setting-up-spark-in-docker" id="setting-up-spark-in-docker">Setting up Spark in Docker</a></h3>
<p>In order to run Spark in a container, a <code>Dockerfile</code> is provided in the root of
all repositories we will use in this lab, including the repository for the
Getting Started guide. The <code>Dockerfile</code> can be used to build images for
<code>spark-submit</code> to run your Spark application, <code>spark-shell</code> to run a Spark
interactive shell, and the Spark history server to view event logs from
application runs. You need to build these images before you get started. The
Dockerfiles we provide assume that you run Docker from the folder at which they
are located. Don't move them around! They will stop working.</p>
<p>To build a docker image from the Dockerfile, we use <code>docker build</code>:</p>
<pre><code class="language-bash">docker build --target &lt;target&gt; -t &lt;tag&gt; .
</code></pre>
<p>Here <code>&lt;target&gt;</code> selects the target from the Dockerfile, <code>&lt;tag&gt;</code> sets the tag
for the resulting image, and the <code>.</code> sets the build context to the current
working directory.</p>
<p>We use <code>docker build</code> to build the images we need to use Spark and SBT.</p>
<ul>
<li>
<p><code>sbt</code></p>
<pre><code class="language-bash">docker build \
--build-arg BASE_IMAGE_TAG=&quot;8&quot; \
--build-arg SBT_VERSION=&quot;1.3.13&quot; \
--build-arg SCALA_VERSION=&quot;2.12.12&quot; \
-t sbt \
github.com/hseeberger/scala-sbt.git#:debian
</code></pre>
</li>
<li>
<p><code>spark-shell</code></p>
<pre><code class="language-bash">docker build --target spark-shell -t spark-shell .
</code></pre>
</li>
<li>
<p><code>spark-submit</code></p>
<pre><code class="language-bash">docker build --target spark-submit -t spark-submit .
</code></pre>
</li>
<li>
<p><code>spark-history-server</code></p>
<pre><code class="language-bash">docker build --target spark-history-server -t spark-history-server .
</code></pre>
</li>
</ul>
<p>You could then run the following commands from the Spark application root
(the folder containing the <code>build.sbt</code> file). Please make sure to use the
provided template project.</p>
<p>The commands below are provided as a reference, and they will be used throughout
the rest of this guide. You do not have to run them now, because some of them
require additional parameters (e.g. <code>spark-submit</code>) that we will provide later
in the manual.</p>
<ul>
<li>
<p>To run SBT to package or test your application (<code>sbt &lt;command&gt;</code>)</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/root sbt sbt
</code></pre>
</li>
<li>
<p>To start a Spark shell (<code>spark-shell</code>)</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io spark-shell
</code></pre>
</li>
<li>
<p>To run your Spark application (<code>spark-submit</code>) (fill in the class name of your
application and the name of your project!)</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io -v &quot;`pwd`&quot;/spark-events:/spark-events \
spark-submit --class &lt;YOUR_CLASSNAME&gt; \
target/scala-2.12/&lt;YOUR_PROJECT_NAME&gt;_2.12-1.0.jar
</code></pre>
</li>
<li>
<p>To spawn the history server to view event logs, accessible at
<a href="http://localhost:18080">localhost:18080</a></p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;/spark-events:/spark-events \
-p 18080:18080 spark-history-server
</code></pre>
</li>
</ul>
<p>The further we get in the manual, we will generally not mention the full Docker
commands this explicitly again, so know that if we mention e.g. <code>spark-shell</code>,
you should run the corresponding <code>docker run</code> command listed above. You can
create scripts or aliases for your favorite shell to avoid having to type a lot.</p>
<h2><a class="header" href="#scala-1" id="scala-1">Scala</a></h2>
<p>Apache Spark, our big data framework of choice for this lab, is implemented in
Scala, a compiled language on the JVM that supports a mix between functional
and object-oriented programming. It is compatible with Java libraries. Some
reasons why Spark was written in Scala are:</p>
<ol>
<li>
<p>Compiling to the JVM makes the codebase extremely portable and deploying
applications as easy as sending the Java bytecode (typically packaged in a
<strong>J</strong>ava <strong>AR</strong>chive format, or JAR). This simplifies deploying to cloud
provider big data platforms as we don't need specific knowledge of the
operating system, or even the underlying architecture.</p>
</li>
<li>
<p>Compared to Java, Scala has some advantages in supporting more complex
types, type inference, and anonymous functions (Since Java 8, Java also supports anonymous functions, or
lambda expression, but this version wasn't released at the time of Spark's
initial release.). Matei
Zaharia, Apache Spark's original author, has said the following about why
Spark was implemented in Scala in a <a href="https://www.reddit.com/r/IAmA/comments/31bkue/im_matei_zaharia_creator_of_spark_and_cto_at/">Reddit AMA</a>:</p>
<blockquote>
<p>At the time we started, I really wanted a PL that supports a
language-integrated interface (where people write functions inline, etc),
because I thought that was the way people would want to program these
applications after seeing research systems that had it (specifically
Microsoft's DryadLINQ). However, I also wanted to be on the JVM in order to
easily interact with the Hadoop filesystem and data formats for that. Scala
was the only somewhat popular JVM language then that offered this kind of
functional syntax and was also statically typed (letting us have some control
over performance), so we chose that. Today there might be an argument to make
the first version of the API in Java with Java 8, but we also benefitted from
other aspects of Scala in Spark, like type inference, pattern matching, actor
libraries, etc.</p>
</blockquote>
</li>
</ol>
<p>Apache Spark provides interfaces to Scala, R, Java and Python, but we will be
using Scala to program in this lab. An introduction to Scala can be found on
the <a href="https://docs.scala-lang.org/tour/tour-of-scala.html">Scala language site</a>.
You can have a brief look at it, but you can also pick up topics as you go through the lab.</p>
<h2><a class="header" href="#apache-spark-1" id="apache-spark-1">Apache Spark</a></h2>
<p>Apache Spark provides a programming model for a resilient distributed
shared memory model. To elaborate on this, Spark allows you to program against
a <em>unified view</em> of memory (i.e. RDD or DataFrame), while the processing
happens <em>distributed</em> over <em>multiple nodes/machines/computers/servers</em> being
able to compensate for <em>failures of these nodes</em>.</p>
<p>This allows us to define a computation and scale this over multiple machines
without having to think about communication, distribution of data, and
potential failures of nodes. This advantage comes at a cost: All applications
have to comply with Spark's (restricted) programming model.</p>
<p>The programming model Spark exposes is based around the MapReduce paradigm.
This is an important consideration when you would consider using Spark, does my
problem fit into this paradigm?</p>
<p>Modern Spark exposes two APIs around this programming model:</p>
<ol>
<li>Resilient Distributed Datasets</li>
<li>Spark SQL Dataframe/Datasets</li>
</ol>
<p>In the rest of this section, we will demonstrate a simple application with
implementations using both APIs.</p>
<h3><a class="header" href="#resilient-distributed-datasets" id="resilient-distributed-datasets">Resilient Distributed Datasets</a></h3>
<p><img src="getting-started/apache-spark/../../assets/images/RDD.png" alt="Illustration of RDD abstraction of an RDD with a tuple of characters and integers as elements." /></p>
<blockquote>
<p>Illustration of RDD abstraction of an RDD with a tuple of characters and integers as elements.</p>
</blockquote>
<p>RDDs are the original data abstraction used in Spark. Conceptually one can
think of these as a large, unordered list of Java/Scala/Python objects, let's
call these objects elements. This list of elements is divided in partitions
(which may still contain multiple elements), which can reside on different
machines. One can operate on these elements with a number of operations, which
can be subdivided in wide and narrow dependencies, see the table below. An
illustration of the RDD abstraction can be seen in the figure above.</p>
<p>RDDs are immutable, which means that the elements cannot be altered, without
creating a new RDD. Furthermore, the application of transformations (wide or
narrow) is <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation</a>,
meaning that the actual computation will be delayed until results are requested
(an action in Spark terminology). When applying transformations, these will
form a directed acyclic graph (DAG), that instructs workers what operations to
perform, on which elements to find a specific result. This can be seen in the
figure above as the arrows between elements.</p>
<table><thead><tr><th align="left">Narrow Dependency</th><th align="left">Wide Dependency</th></tr></thead><tbody>
<tr><td align="left"><code>map</code></td><td align="left"><code>coGroup</code></td></tr>
<tr><td align="left"><code>mapValues</code></td><td align="left"><code>flatMap</code></td></tr>
<tr><td align="left"><code>flatMap</code></td><td align="left"><code>groupByKey</code></td></tr>
<tr><td align="left"><code>filter</code></td><td align="left"><code>reduceByKey</code></td></tr>
<tr><td align="left"><code>mapPartitions</code></td><td align="left"><code>combineByKey</code></td></tr>
<tr><td align="left"><code>mapPartitionsWithIndex</code></td><td align="left"><code>distinct</code></td></tr>
<tr><td align="left"><code>join</code> with sorted keys</td><td align="left"><code>join</code></td></tr>
<tr><td align="left"></td><td align="left"><code>intersection</code></td></tr>
<tr><td align="left"></td><td align="left"><code>repartition</code></td></tr>
<tr><td align="left"></td><td align="left"><code>coalesce</code></td></tr>
<tr><td align="left"></td><td align="left"><code>sort</code></td></tr>
</tbody></table>
<blockquote>
<p>List of wide and narrow dependencies for (pair) RDD operations</p>
</blockquote>
<p>Now that you have an idea of what the abstraction is about, let's demonstrate
some example code with the Spark shell. </p>
<p><em>If you want to paste pieces of code into the spark shell from this guide, it
might be useful to copy from the github version, and use the <code>:paste</code> command in
the spark shell to paste the code. Hit <code>ctrl+D</code> to stop pasting.</em></p>
<p>We can start the shell using <a href="getting-started/apache-spark/../docker.html">Docker</a>:</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io spark-shell
</code></pre>
<p>We should now ge the following output:</p>
<pre><code class="language-scala">1337-42-01 07:44:21,765 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://333c7146e54b:4040
Spark context available as 'sc' (master = local[*], app id = local-1599464666576).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.6
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_265)
Type in expressions to have them evaluated.
Type :help for more information.
</code></pre>
<p>When opening a Spark Shell, by default, you get two objects.</p>
<ul>
<li>A <code>SparkSession</code> named <code>spark</code>.</li>
<li>A <code>SparkContext</code> named <code>sc</code>. </li>
</ul>
<p>These objects contains the configuration of your session, i.e. whether you are
running in local or cluster mode, the name of your application, the logging
level etc.</p>
<p>We can get some (sometimes slightly arcane) information about Scala objects that
exist in the scope of the shell, e.g.:</p>
<pre><code class="language-scala">scala&gt; spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@747e0a31
</code></pre>
<p>Now, let's first create some sample data that we can demonstrate the RDD API
around. Here we create an infinite list of repeating characters from 'a' tot
'z'.</p>
<pre><code class="language-scala">scala&gt; val charsOnce = ('a' to 'z').toStream
charsOnce: scala.collection.immutable.Stream[Char] = Stream(a, ?)

scala&gt; val chars: Stream[Char] = charsOnce #::: chars
chars: Stream[Char] = Stream(a, ?)
</code></pre>
<p>Now we build a collection with the first 200000 integers, zipped with the
character stream. We display the first five results.</p>
<pre><code class="language-scala">scala&gt; val rdd = sc.parallelize(chars.zip(1 to 200000), numSlices=20)
rdd: org.apache.spark.rdd.RDD[(Char, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:26

scala&gt; rdd.take(5)
res2: Array[(Char, Int)] = Array((a,1), (b,2), (c,3), (d,4), (e,5))
</code></pre>
<p>Let's dissect what just happened. We created a Scala object that is a list of
tuples of <code>Char</code>s and <code>Int</code>s in the statement <code>(chars).zip(1 to 200000)</code>. With
<code>sc.parallelize</code> we are transforming a Scala sequence into an RDD. This allows
us to enter Spark's programming model. With the optional parameter <code>numSlices</code>
we indicate in how many partitions we want to subdivide the sequence.</p>
<p>Let's apply some (lazily evaluated) transformations to this RDD.</p>
<pre><code class="language-scala">scala&gt; val mappedRDD = rdd.map({case (chr, num) =&gt; (chr, num+1)})
mappedRDD: org.apache.spark.rdd.RDD[(Char, Int)] = MapPartitionsRDD[1] at map at &lt;console&gt;:25
</code></pre>
<p>We apply a <code>map</code> to the RDD, applying a function to all the elements in the
RDD. The function we apply pattern matches over the elements as being a tuple
of <code>(Char, Int)</code>, and add one to the integer. Scala's syntax can be a bit
foreign, so if this is confusing, spend some time looking at tutorials and
messing around in the Scala interpreter.</p>
<p>You might have noticed that the transformation completed awfully fast. This is
Spark's <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation</a> in
action. No computation will be performed until an action is applied.</p>
<pre><code class="language-scala">scala&gt; val reducedRDD = rdd.reduceByKey(_ + _)
reducedRDD: org.apache.spark.rdd.RDD[(Char, Int)] = ShuffledRDD[2] at reduceByKey at &lt;console&gt;:25
</code></pre>
<p>Now we apply a <code>reduceByKey</code> operation, grouping all of the identical keys together and
merging the results with the specified function, in this case the <code>+</code> operator.</p>
<p>Now we will perform an action, which will trigger the computation of the
transformations on the data. We will use the collect action, which means to
gather all the results to the master, going out of the Spark programming model,
back to a Scala sequence. How many elements do you expect there to be in this
sequence after the previous transformations?</p>
<pre><code class="language-scala">scala&gt; reducedRDD.collect
res3: Array[(Char, Int)] = Array((d,769300000), (x,769253844), (e,769307693),
(y,769261536), (z,769269228), (f,769315386), (g,769323079), (h,769330772),
(i,769138464), (j,769146156), (k,769153848), (l,769161540), (m,769169232),
(n,769176924), (o,769184616), (p,769192308), (q,769200000), (r,769207692),
(s,769215384), (t,769223076), (a,769276921), (u,769230768), (b,769284614),
(v,769238460), (w,769246152), (c,769292307))
</code></pre>
<p>Typically, we don't build the data first, but we actually load it from a
database or file system. Say we have some data in (multiple) files in a
specific format. As an example consider <code>sensordata.csv</code> (in the <code>example</code>
folder). We can load it as follows:</p>
<pre><code class="language-scala">// sc.textFile can take multiple files as argument!
scala&gt; val raw_data = sc.textFile(&quot;sensordata.csv&quot;)
raw_data: org.apache.spark.rdd.RDD[String] = sensordata.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:24
</code></pre>
<p>And observe some of its contents:</p>
<pre><code class="language-scala">scala&gt; raw_data.take(10).foreach(println)
COHUTTA,3/10/14:1:01,10.27,1.73,881,1.56,85,1.94
COHUTTA,3/10/14:1:02,9.67,1.731,882,0.52,87,1.79
COHUTTA,3/10/14:1:03,10.47,1.732,882,1.7,92,0.66
COHUTTA,3/10/14:1:05,9.56,1.734,883,1.35,99,0.68
COHUTTA,3/10/14:1:06,9.74,1.736,884,1.27,92,0.73
COHUTTA,3/10/14:1:08,10.44,1.737,885,1.34,93,1.54
COHUTTA,3/10/14:1:09,9.83,1.738,885,0.06,76,1.44
COHUTTA,3/10/14:1:11,10.49,1.739,886,1.51,81,1.83
COHUTTA,3/10/14:1:12,9.79,1.739,886,1.74,82,1.91
COHUTTA,3/10/14:1:13,10.02,1.739,886,1.24,86,1.79
</code></pre>
<p>We can process this data to filter only measurements on <code>3/10/14:1:01</code>.</p>
<pre><code class="language-scala">scala&gt; val filterRDD = raw_data.map(_.split(&quot;,&quot;))
                               .filter(x =&gt; x(1) == &quot;3/10/14:1:01&quot;)
filterRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[11] at filter at &lt;console&gt;:25
</code></pre>
<p>And look at the output:</p>
<pre><code class="language-scala">scala&gt; filterRDD.foreach(a =&gt; println(a.mkString(&quot; &quot;)))
COHUTTA 3/10/14:1:01 10.27 1.73 881 1.56 85 1.94
LAGNAPPE 3/10/14:1:01 9.59 1.602 777 0.09 88 1.78
NANTAHALLA 3/10/14:1:01 10.47 1.712 778 1.96 76 0.78
CHER 3/10/14:1:01 10.17 1.653 777 1.89 96 1.57
THERMALITO 3/10/14:1:01 10.24 1.75 777 1.25 80 0.89
ANDOUILLE 3/10/14:1:01 10.26 1.048 777 1.88 94 1.66
BUTTE 3/10/14:1:01 10.12 1.379 777 1.58 83 0.67
CARGO 3/10/14:1:01 9.93 1.903 778 0.55 76 1.44
MOJO 3/10/14:1:01 10.47 1.828 967 0.36 77 1.75
BBKING 3/10/14:1:01 10.03 0.839 967 1.17 80 1.28
</code></pre>
<p>You might have noticed that this is a bit tedious to work with, as we have to
convert everything to Scala objects, and aggregations rely on having a pair RDD,
which is fine when we have a single key. For more complex aggregations, this
becomes a bit tedious to juggle with.</p>
<h3><a class="header" href="#dataframe-and-dataset" id="dataframe-and-dataset">Dataframe and Dataset</a></h3>
<p>Our previous example is quite a typical use case for Spark. We have a big data
store of some structured (tabular) format (be it csv, JSON, parquet, or
something else) that we would like to analyse, typically in some SQL-like
fashion. Manually applying operations to rows like this is both labour
intensive, and inefficient, as we have knowledge of the 'schema' of data. This
is where DataFrames originate from. Spark has an optimized SQL query engine that
can optimize the compute path as well as provide a more efficient representation
of the rows when given a schema. From the
<a href="https://spark.apache.org/docs/2.4.6/sql-programming-guide.html#overview">Spark SQL, DataFrames and Datasets
Guide</a>:</p>
<blockquote>
<p>Spark SQL is a Spark module for structured data processing. Unlike the basic
Spark RDD API, the interfaces provided by Spark SQL provide Spark with more
information about the structure of both the data and the computation being
performed. Internally, Spark SQL uses this extra information to perform extra
optimizations. There are several ways to interact with Spark SQL including
SQL and the Dataset API. When computing a result the same execution engine is
used, independent of which API/language you are using to express the
computation. This unification means that developers can easily switch back
and forth between different APIs based on which provides the most natural way
to express a given transformation.</p>
</blockquote>
<p>Under the hood, these are still immutable distributed collections of data (with
the same compute graph semantics, only now Spark can apply extra
optimizations because of the (structured) format.</p>
<p>Let's do the same analysis as last time using this API. First we will define a
schema. Let's take a look at a single row of the csv:</p>
<pre><code>COHUTTA,3/10/14:1:01,10.27,1.73,881,1.56,85,1.94
</code></pre>
<p>So first a string field, a date, a timestamp, and some numeric information.
We can thus define the schema as such:</p>
<pre><code class="language-scala">val schema =
  StructType(
    Array(
      StructField(&quot;sensorname&quot;, StringType, nullable=false),
      StructField(&quot;timestamp&quot;, TimestampType, nullable=false),
      StructField(&quot;numA&quot;, DoubleType, nullable=false),
      StructField(&quot;numB&quot;, DoubleType, nullable=false),
      StructField(&quot;numC&quot;, LongType, nullable=false),
      StructField(&quot;numD&quot;, DoubleType, nullable=false),
      StructField(&quot;numE&quot;, LongType, nullable=false),
      StructField(&quot;numF&quot;, DoubleType, nullable=false)
    )
  )
</code></pre>
<p>If we import types first, and then enter this in our interactive shell we get
the following:</p>
<pre><code class="language-scala">scala&gt; :paste
// Entering paste mode (ctrl-D to finish)
import org.apache.spark.sql.types._

val schema =
  StructType(
    Array(
      StructField(&quot;sensorname&quot;, StringType, nullable=false),
      StructField(&quot;timestamp&quot;, TimestampType, nullable=false),
      StructField(&quot;numA&quot;, DoubleType, nullable=false),
      StructField(&quot;numB&quot;, DoubleType, nullable=false),
      StructField(&quot;numC&quot;, LongType, nullable=false),
      StructField(&quot;numD&quot;, DoubleType, nullable=false),
      StructField(&quot;numE&quot;, LongType, nullable=false),
      StructField(&quot;numF&quot;, DoubleType, nullable=false)
    )
  )


// Exiting paste mode, now interpreting.

import org.apache.spark.sql.types._
schema: org.apache.spark.sql.types.StructType =
StructType(StructField(sensorname,StringType,false),
StructField(timestamp,TimestampType,false), StructField(numA,DoubleType,false),
StructField(numB,DoubleType,false), StructField(numC,LongType,false),
StructField(numD,DoubleType,false), StructField(numE,LongType,false),
StructField(numF,DoubleType,false))
</code></pre>
<p>An overview of the different <a href="https://spark.apache.org/docs/2.4.6/api/scala/#org.apache.spark.sql.types.package">Spark SQL types</a>
can be found online. For the timestamp field we need to specify the format
according to the <a href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html">Javadate format</a>
in our case <code>MM/dd/yy:hh:mm</code>. Tying this all together we can build a Dataframe
like so.</p>
<pre><code class="language-scala">scala&gt; :paste
// Entering paste mode (ctrl-D to finish)
val df = spark.read
              .schema(schema)
              .option(&quot;timestampFormat&quot;, &quot;MM/dd/yy:hh:mm&quot;)
              .csv(&quot;./sensordata.csv&quot;)
// Exiting paste mode, now interpreting.
df: org.apache.spark.sql.DataFrame =
        [sensorname: string, timestamp: date ... 6 more fields]

scala&gt; df.printSchema
root
 |-- sensorname: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- numA: double (nullable = true)
 |-- numB: double (nullable = true)
 |-- numC: long (nullable = true)
 |-- numD: double (nullable = true)
 |-- numE: long (nullable = true)
 |-- numF: double (nullable = true

scala&gt; df.take(5).foreach(println)
[COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94]
[COHUTTA,2014-03-10 01:02:00.0,9.67,1.731,882,0.52,87,1.79]
[COHUTTA,2014-03-10 01:03:00.0,10.47,1.732,882,1.7,92,0.66]
[COHUTTA,2014-03-10 01:05:00.0,9.56,1.734,883,1.35,99,0.68]
[COHUTTA,2014-03-10 01:06:00.0,9.74,1.736,884,1.27,92,0.73]
</code></pre>
<p>We will now continue to perform the same filtering operation as previously
performed on the RDD.
There are three ways in which we could do this:</p>
<ol>
<li>By supplying an SQL query string to Spark SQL, operating on the <em>untyped</em>
<code>DataFrame</code>.</li>
<li>By using the Scala API for the <em>untyped</em> <code>DataFrame</code>.</li>
<li>By using the Scala API for the <em>strongly-typed</em> <code>DataSet</code>. </li>
</ol>
<h4><a class="header" href="#sql-query-string" id="sql-query-string">SQL query string</a></h4>
<p>We can use really error prone SQL queries, like so:</p>
<pre><code class="language-scala">scala&gt; df.createOrReplaceTempView(&quot;sensor&quot;)

scala&gt; val dfFilter = spark.sql(&quot;SELECT * FROM sensor
WHERE timestamp=TIMESTAMP(\&quot;2014-03-10 01:01:00\&quot;)&quot;)
// I think the newline in the multiline string breaks it if you paste it
dfFilter: org.apache.spark.sql.DataFrame =
            [sensorname: string, timestamp: timestamp ... 6 more fields]

scala&gt; dfFilter.collect.foreach(println)
[COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94]
[NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78]
[THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89]
...
</code></pre>
<p>As you can see, we're simply providing an SQL string to the <code>spark.sql</code> method.
The string is not checked by the Scala compiler, but only during run-time by
the Spark SQL library. Any errors will only show up during run-time.
These errors may include both typos in </p>
<ul>
<li>the SQL keywords, and</li>
<li>the field names, and</li>
<li>the timestamp.</li>
</ul>
<p>This is not recommended unless you absolutely love SQL and like debugging these
command strings. (This took me about 20 minutes to get right!)</p>
<h4><a class="header" href="#dataframe" id="dataframe">DataFrame</a></h4>
<p>A slightly more sane and type-safe way would be to do the following:</p>
<pre><code class="language-scala">scala&gt; val dfFilter = df.filter(&quot;timestamp = TIMESTAMP(\&quot;2014-03-10 01:01:00\&quot;)&quot;)
dfFilter: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] =
                    [sensorname: string, timestamp: timestamp ... 6 more fields]

scala&gt; dfFilter.collect.foreach(println)
[COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94]
[NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78]
[THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89]
...
</code></pre>
<p>We have now replaced the SQL query of the form:</p>
<pre><code class="language-sql">SELECT fieldname WHERE predicate
</code></pre>
<p>... with the <code>filter()</code> method of a Spark DataFrame. This is already a bit
better, since the <em>Scala compiler</em> and <em>not the Spark SQL run-time library</em> can
now check the the existence of the <code>filter()</code> method for the DataFrame class.
If we made a typo, we would get a compiler error, before running the code!</p>
<p>Also, the methods supported by DataFrames look much like those of 
<a href="https://docs.scala-lang.org/overviews/parallel-collections/overview.html">Scala's parallel collections</a>,
just like with RDDs, but there are also some SQL-like database-oriented methods
such as <code>join()</code>. As such, the Scala API for DataFrames combines the best of
both worlds.</p>
<p>Still, this approach is error-prone, since it is allowed to write the filter
predicate as an SQL predicate, retaining the problem of potential errors in the
timestamp field name and the timestamp itself.</p>
<h4><a class="header" href="#dataset" id="dataset">DataSet</a></h4>
<p>Luckily, there is also the DataSet abstraction. 
It is a sort of middle ground between DataFrames and RDDs, where you get some of
the type safety of RDDs by operating on a Scala <a href="https://docs.scala-lang.org/tour/case-classes.html">case
class</a> (also known as
product type).</p>
<p>This allows even more compile-time type checking on the product types, while
still allowing Spark to optimize the query and storage of the data by making use
of schemas.</p>
<p>We do have to write a bit more Scala to be able to use the strongly-typed
DataSet:</p>
<pre><code class="language-scala">scala&gt; import java.sql.Timestamp
import java.sql.Timestamp

scala&gt; :paste
// Entering paste mode (ctrl-D to finish)

case class SensorData (
    sensorName: String,
    timestamp: Timestamp,
    numA: Double,
    numB: Double,
    numC: Long,
    numD: Double,
    numE: Long,
    numF: Double
)

// Exiting paste mode, now interpreting.

defined class SensorData
</code></pre>
<p>Now we can convert a DataFrame (which is actually just a <code>DataSet[Row]</code>, where
<code>Row</code> allows fields to be untyped) to a typed DataSet using the <code>as</code> method.</p>
<pre><code class="language-scala">scala&gt; :paste
// Entering paste mode (ctrl-D to finish)

val ds = spark.read
              .schema(schema)
              .option(&quot;timestampFormat&quot;, &quot;MM/dd/yy:hh:mm&quot;)
              .csv(&quot;./sensordata.csv&quot;)
              .as[SensorData]

// Exiting paste mode, now interpreting.

ds: org.apache.spark.sql.Dataset[SensorData] =
            [sensorname: string, timestamp: timestamp ... 6 more fields]
</code></pre>
<p>Now we can apply compile-time type-checked operations:</p>
<pre><code class="language-scala">scala&gt; val dsFilter = ds.filter(a =&gt; a.timestamp ==
                                new Timestamp(2014 - 1900, 2, 10, 1, 1, 0, 0))
dsFilter: org.apache.spark.sql.Dataset[SensorData] =
                [sensorname: string, timestamp: timestamp ... 6 more fields]

scala&gt; dsFilter.collect.foreach(println)
SensorData(COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94)
SensorData(NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78)
SensorData(THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89)
...
</code></pre>
<p>This has two advantages:</p>
<ul>
<li>The field names can now be checked by the Scala compiler as well, by inspecting
our case class. It will detect if we made a mistake when writing <code>a.timestamp</code>.</li>
<li>The SQL-like predicate used in the DataFrame implementation is now
replaced with the constructor of the Timestamp class. This is more type-safe,
since any type mismatches will be detected by the Scala compiler. </li>
</ul>
<p>Of course, you could still supply an incorrect month number (e.g. 13). However,
the Timestamp object is already created during construction of the
lazy-evaluated DAG, not when that stage of the DAG actually starts computation.
The constructor will therefore raise any errors early on, and not at the end
stage of e.g. some computation that has been taking hours already.</p>
<p>We now have an setup where the Scala compiler will check all methods used to
build up the directed acyclic graph (DAG) of our computation exist at every
intermediate resulting DataFrame. We can't make mistakes in the SQL keywords
anymore, as well as mistakes in the field names, or data types thrown into the
filter. This provides us with more guarantees that our queries are valid (at
least at the type level).</p>
<p>There are a lot of additional advantages to DataSets that have not yet been
exposed through these examples. DataBricks has published <a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">an excellent
blog</a>
about why DataSets were introduced, next to RDDs. While DataSets don't replace
RDDs, they are nowadays most often used, because they have some more nice
properties as explained before. Read the blog to get to know the details!</p>
<p>This was a brief overview of the 2 (or 3) different Spark APIs. You can always
find more information on the programming guides for
<a href="https://spark.apache.org/docs/2.4.6/rdd-programming-guide.html">RDDs</a> and
<a href="https://spark.apache.org/docs/2.4.6/sql-programming-guide.html">Dataframes/Datasets</a>
and in the <a href="https://spark.apache.org/docs/2.4.6/api/scala/index.html#package">Spark
documentation</a>.</p>
<h3><a class="header" href="#packaging-your-application-using-sbt" id="packaging-your-application-using-sbt">Packaging your application using SBT</a></h3>
<p>We showed how to run Spark in interactive mode. Now we will explain how to build
applications that can be submitted using the <code>spark-submit</code> command.</p>
<p>First, we will explain how to structure a Scala project, using the <a href="https://www.scala-sbt.org">SBT build
tool</a>. The typical project structure is</p>
<pre><code> build.sbt
 project
    build.properties
 src
     main
         scala
             example.scala
</code></pre>
<p>This is typical for JVM languages. More directories are added under the <code>scala</code>
folder to resemble the package structure.</p>
<p>The project's name, dependencies, and versioning is defined in the <code>build.sbt</code>
file. An example <code>build.sbt</code> file is</p>
<pre><code class="language-scala">name := &quot;Example&quot;
version := &quot;0.1.0&quot;
scalaVersion := &quot;2.12.12&quot;
</code></pre>
<p>This specifies the Scala version of the project (2.12.12) and the name of the
project.</p>
<p>If you run <code>sbt</code> in this folder it will generate the project directory and
<code>build.properties</code>. <code>build.properties</code> contains the SBT version that is
used to build the project with, for backwards compatibility.</p>
<p>Open <code>example.scala</code> and add the following</p>
<pre><code class="language-scala">package example


object Example {
  def main(args: Array[String]) {
    println(&quot;Hello world!&quot;)
  }
}
</code></pre>
<p>Start the <code>sbt</code> container in the root folder (the one where <code>build.sbt</code> is
located). This puts you in interactive mode of SBT. We can compile the sources
by writing the <code>compile</code> command.</p>
<pre><code>$ docker run -it --rm -v &quot;`pwd`&quot;:/root sbt sbt
[info] [launcher] getting org.scala-sbt sbt 1.3.13  (this may take some time)...
downloading https://repo1.maven.org/maven2/org/scala-sbt/sbt/1.3.13/sbt-1.3.13.jar ...
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar ...

...
https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.3.0/scala-xml_2.12-1.3.0.jar
  100.0% [##########] 544.5 KiB (4.8 MiB / s)
[info] Fetched artifacts of
[info] loading settings for project root from build.sbt ...
[info] set current project to Example (in build file:/root/)
[info] sbt server started at local:///root/.sbt/1.0/server/27dc1aa3fdf4049b492d/sock
sbt:Example&gt;
</code></pre>
<p>We can now type <code>compile</code>.</p>
<pre><code>sbt:Example&gt; compile
[info] Updating
[info] Resolved  dependencies
[info] Updating
https://repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.12/scala-compiler-2.12.12.pom
  100.0% [##########] 2.6 KiB (69.1 KiB / s)

...

[info] Compiling 1 Scala source to /root/target/scala-2.12/classes ...
[info] Non-compiled module 'compiler-bridge_2.12' for Scala 2.12.12. Compiling...
[info]   Compilation completed in 8.038s.
[success] Total time: 10 s, completed Sep 7, 2020 10:39:38 AM

</code></pre>
<p>We can try to run the application by typing <code>run</code>.</p>
<pre><code>sbt:Example&gt; run
[info] running example.Example
Hello world!
[success] Total time: 0 s, completed Sep 7, 2020 10:40:24 AM
</code></pre>
<p>Now let's add a function to <code>example.scala</code>.</p>
<pre><code class="language-scala">object Example {
  def addOne(tuple: (Char, Int)) : (Char, Int) = tuple match {
    case (chr, int) =&gt; (chr, int+1)
  }
  def main(args: Array[String]) {
    println(&quot;Hello world!&quot;)
    println(addOne('a', 1))
  }
}
</code></pre>
<p>In your SBT session we can prepend any command with a tilde (<code>~</code>) to make them
run automatically on source changes.</p>
<pre><code>sbt:Example&gt; ~run
[info] Compiling 1 Scala source to /root/target/scala-2.12/classes ...
[info] running example.Example
Hello world!
(a,2)
[success] Total time: 0 s, completed Sep 7, 2020 10:40:56 AM
[info] 1. Monitoring source files for root/run...
[info]    Press &lt;enter&gt; to interrupt or '?' for more options.

</code></pre>
<p>We can also open an interactive session using SBT.</p>
<pre><code>sbt:Example&gt; console
[info] Starting scala interpreter...
Welcome to Scala 2.12.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_265).
Type in expressions for evaluation. Or try :help.

scala&gt; example.Example.addOne('a', 1)
res1: (Char, Int) = (a,2)

scala&gt; println(&quot;Interactive environment&quot;)
Interactive environment
</code></pre>
<p>To build Spark applications with SBT we need to include dependencies (Spark
most notably) to build the project. Modify your <code>build.sbt</code> file like so</p>
<pre><code class="language-scala">name := &quot;Example&quot;
version := &quot;0.1.0&quot;
scalaVersion := &quot;2.12.12&quot;

val sparkVersion = &quot;2.4.6&quot;

libraryDependencies ++= Seq(
  &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % sparkVersion,
  &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % sparkVersion
)
</code></pre>
<p>If you still have the SBT shell opened, you must use <code>reload</code> to make sure your
<code>build.sbt</code> is updated.</p>
<p>We could now use Spark in the script (after running <code>compile</code>).</p>
<p>Let's implement a Spark application.
Modify <code>example.scala</code> as follows, but don't run the code yet!</p>
<pre><code class="language-scala">package example

import org.apache.spark.sql.types._
import org.apache.spark.sql._
import java.sql.Timestamp


object ExampleSpark {
  case class SensorData (
    sensorName: String,
    timestamp: Timestamp,
    numA: Double,
    numB: Double,
    numC: Long,
    numD: Double,
    numE: Long,
    numF: Double
  )
  def main(args: Array[String]) {
    val schema =
      StructType(
        Array(
          StructField(&quot;sensorname&quot;, StringType, nullable=false),
          StructField(&quot;timestamp&quot;, TimestampType, nullable=false),
          StructField(&quot;numA&quot;, DoubleType, nullable=false),
          StructField(&quot;numB&quot;, DoubleType, nullable=false),
          StructField(&quot;numC&quot;, LongType, nullable=false),
          StructField(&quot;numD&quot;, DoubleType, nullable=false),
          StructField(&quot;numE&quot;, LongType, nullable=false),
          StructField(&quot;numF&quot;, DoubleType, nullable=false)
        )
      )

    val spark = SparkSession
      .builder
      .appName(&quot;Example&quot;)
      .getOrCreate()
    val sc = spark.sparkContext // If you need SparkContext object

    import spark.implicits._

    val ds = spark.read
                  .schema(schema)
                  .option(&quot;timestampFormat&quot;, &quot;MM/dd/yy:hh:mm&quot;)
                  .csv(&quot;./sensordata.csv&quot;)
                  .as[SensorData]

    val dsFilter = ds.filter(a =&gt; a.timestamp ==
        new Timestamp(2014 - 1900, 2, 10, 1, 1, 0, 0))

    dsFilter.collect.foreach(println)

    spark.stop
  }
}
</code></pre>
<p>We will not run this code, but submit it to a local Spark &quot;cluster&quot; (on your
machine). To do so, we require a JAR. You can build a JAR using the <code>package</code>
command in SBT. This JAR will be located in the
<code>target/scala-version/project_name_version.jar</code>.</p>
<p>You can run the JAR via a <code>spark-submit</code> container (which will run on local
mode). By mounting the <code>spark-events</code> directory the event log of the
application run is stored to be inspected later using the Spark history server.</p>
<pre><code>$ docker run -it --rm -v &quot;`pwd`&quot;:/io -v &quot;`pwd`&quot;/spark-events:/spark-events spark-submit target/scala-2.12/example_2.12-0.1.0.jar
</code></pre>
<p>The output should look as follows:</p>
<pre><code>2020-09-07 11:07:28,890 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-07 11:07:29,068 INFO spark.SparkContext: Running Spark version 2.4.6
2020-09-07 11:07:29,087 INFO spark.SparkContext: Submitted application: Example

...

SensorData(COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94)
SensorData(NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78)
SensorData(THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89)
SensorData(BUTTE,2014-03-10 01:01:00.0,10.12,1.379,777,1.58,83,0.67)
SensorData(CARGO,2014-03-10 01:01:00.0,9.93,1.903,778,0.55,76,1.44)
SensorData(LAGNAPPE,2014-03-10 01:01:00.0,9.59,1.602,777,0.09,88,1.78)
SensorData(CHER,2014-03-10 01:01:00.0,10.17,1.653,777,1.89,96,1.57)
SensorData(ANDOUILLE,2014-03-10 01:01:00.0,10.26,1.048,777,1.88,94,1.66)
SensorData(MOJO,2014-03-10 01:01:00.0,10.47,1.828,967,0.36,77,1.75)
SensorData(BBKING,2014-03-10 01:01:00.0,10.03,0.839,967,1.17,80,1.28)

...

2020-09-07 11:07:33,694 INFO util.ShutdownHookManager: Shutdown hook called
2020-09-07 11:07:33,694 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-757daa7c-c317-428e-934f-aaa9e74bf808
2020-09-07 11:07:33,696 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a38554ba-18fc-46aa-aa1e-0972e24a4cb0
</code></pre>
<p>By default, Spark's logging is quite verbose. You can change the <a href="https://stackoverflow.com/questions/27781187/how-to-stop-info-messages-displaying-on-spark-console">log levels
to warn</a>
to reduce the output.</p>
<p>For development purposes you can also try running the application from SBT
using the <code>run</code> command. Make sure to set the Spark master to <code>local</code> in your
code. You might run into some trouble with threads here, which can be solved
by running the application in a forked process, which can be enabled by
setting <code>fork in run := true</code> in <code>build.sbt</code>. You will also have to set to
change the log levels programmatically, if desired.</p>
<pre><code class="language-scala">import org.apache.log4j.{Level, Logger}
...


def main(args: Array[String]) {
    ...
    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
    ...
}
</code></pre>
<p>You can also use this logger to log your application which might be helpful for
debugging on the AWS cluster later on.</p>
<p>You can inspect the event log from the application run using the Spark history
server. Start a <code>spark-history-server</code> container from the project root folder
and mount the <code>spark-events</code> folder in the container.</p>
<pre><code>$ docker run -it --rm -v &quot;`pwd`&quot;/spark-events/:/spark-events -p 18080:18080 spark-history-server
</code></pre>
<p>The output will look as follows:</p>
<pre><code>starting org.apache.spark.deploy.history.HistoryServer, logging to /spark/logs/spark--org.apache.spark.deploy.history.HistoryServer-1-5b5de5805769.out

...

2020-09-07 11:10:23,020 INFO history.FsHistoryProvider: Parsing file:/spark-events/local-1599477015931 for listing data... 2020-09-07
11:10:23,034 INFO history.FsHistoryProvider: Finished parsing file:/spark-events/local-1599477015931
</code></pre>
<p>Navigate to <a href="localhost:18080">http://localhost:18080</a> to view detailed
information about your jobs.
After analysis you can shutdown the Spark history server using ctrl+C.</p>
<pre><code>^C
2020-09-07 11:13:21,619 ERROR history.HistoryServer: RECEIVED SIGNAL INT
2020-09-07 11:13:21,630 INFO server.AbstractConnector: Stopped Spark@70219bf{HTTP/1.1,[http/1.1]}{0.0.0.0:18080}
2020-09-07 11:13:21,633 INFO util.ShutdownHookManager: Shutdown hook called
</code></pre>
<p>Be sure to explore the history server thoroughly! You can use it to gain an
understanding of how Spark executes your application, as well as to debug and
time your code, which is important for both lab 1 and 2.</p>
<h2><a class="header" href="#amazon-web-services-1" id="amazon-web-services-1">Amazon Web Services</a></h2>
<p>AWS consists of a variety of different services, the ones relevant for this lab
are listed below:</p>
<h3><a class="header" href="#ec2" id="ec2">EC2</a></h3>
<p><a href="https://aws.amazon.com/ec2/">Elastic Compute Cloud</a> allows you to provision a variety of different
machines that can be used to run a computation. An overview of the
different machines and their use cases can be found on the EC2 website.</p>
<h3><a class="header" href="#emr" id="emr">EMR</a></h3>
<p><a href="https://aws.amazon.com/emr/">Elastic MapReduce</a> is a layer on top of EC2, that allows you to quickly
deploy MapReduce-like applications, for instance Apache Spark.</p>
<h3><a class="header" href="#s3" id="s3">S3</a></h3>
<p><a href="https://aws.amazon.com/s3/">Simple Storage Server</a> is an object based storage system that is easy to
interact with from different AWS services.</p>
<p>Note that the OpenStreetMap data is hosted on AWS S3 in the <code>us-east-1</code> region, so any EC2/EMR instances interacting with this data set should also be provisioned
there. At the time of writing, this means that you should select the
N. Virginia region for your instances.</p>
<p>AWS EC2 offers spot instances, a marketplace for unused machines that you can
bid on. These spot instances are often a order of magnitude cheaper than
on-demand instances. The current price list can be found in the <a href="https://aws.amazon.com/ec2/spot/pricing/">EC2 website</a>.
We recommend using spot instances for the entirety of this lab.</p>
<p>We will be using the AWS infrastructure to run the application. Log in to the AWS
console, and open the S3 interface. Create a bucket where we can store the
application JAR, and all the other files needed by your application.</p>
<p>There are (at least) two ways to transfer files to S3:</p>
<ol>
<li>The web interface, and</li>
<li>The command line interface.</li>
</ol>
<p>The web interface is straightforward to use. To use the command line interface,
first install the <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">AWS CLI</a>.
Some example operations are listed below.</p>
<p>To copy a file</p>
<pre><code class="language-bash">aws s3 cp path/to/file s3://destination-bucket/path/to/file
</code></pre>
<p>To copy a directory recursively</p>
<pre><code class="language-bash">aws s3 cp --recursive s3://origin-bucket/path/to/file
</code></pre>
<p>To move a file</p>
<pre><code class="language-bash">aws s3 mv path/to/file s3://destination-bucket/path/to/file
</code></pre>
<p>The aws-cli contains much more functionality, which can be found on the
<a href="https://aws.amazon.com/cli/">AWS-CLI docs</a>.</p>
<p>Once you have uploaded all the necessary files (again your application JAR, and
all the files required by the application), we are ready to provision a
cluster. Go to the EMR service, and select <em>Create Cluster</em>. Next select <em>Go to
advanced options</em>, select the latest release, and check the frameworks you want
to use. In this case this means Spark, Hadoop and Ganglia. Spark and Hadoop you
already know, we will introduce Ganglia later in this chapter.</p>
<p>EMR works with steps, which can be thought of as a job, or the execution of a
single application. You can choose to add steps in the creation of the cluster,
but this can also be done at a later time. Press <em>next</em>.</p>
<p>In the <em>Hardware Configuration</em> screen, we can configure the arrangement and
selection of the machines. We suggest starting out with <code>m4.large</code> machines on
spot pricing. You should be fine running a small example workload with a single
master node and two core nodes. Be sure to select <em>spot pricing</em> and
place an appropriate bid. Remember that you can always check the current prices
in the information popup or on the <a href="https://aws.amazon.com/ec2/spot/pricing/">ec2 website</a>.
After selecting the machines, press <em>next</em>.</p>
<p>Please note:</p>
<ul>
<li>
<p>You always need a master node, which is tasked with distributing
resources and managing tasks for the core nodes. We recommend using
the cheap <code>m4.large</code> instance. If you start to notice unexplained
bottlenecks for tasks with many machines and a lot of data, you might want
to try a larger master node. Ganglia should provide you with some insights
regarding this matter.</p>
</li>
<li>
<p>By default, there are some limitations on the number of spot instances
your account is allowed to provision. If you don't have access to enough
spot instances, the procedure to request additional can be found in the
<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-limits.html">AWS documentation</a>.</p>
</li>
</ul>
<p>In the <em>General Options</em> you can select a cluster name. You can tune where the
system logs and a number of other features (more information in the popups).
After finishing this step, press <em>next</em>.</p>
<p>You should now arrive in the <em>Security Options</em> screen. If you have not created
an <em>EC2 keypair</em>, it is recommended that you do so now. This will allow you to
access the Yarn, Spark, and Ganglia <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html">web interfaces</a>
in your browser. This makes debugging and monitoring the execution of your Spark Job much more manageable.
To create an <em>EC2 keypair</em>, follow <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html">these instructions</a>.</p>
<p>After this has all been completed you are ready to spin up your first cluster
by pressing <em>Create cluster</em>. Once the cluster has been created, AWS will start
provisioning machines. This should take about 10 minutes. In the meantime you
can add a step. Go to the <em>Steps</em> foldout, and select <em>Spark application</em> for
<em>Step Type</em>. Clicking on <em>Configure</em> will open a dialogue in which you can
select the application JAR location in your S3 bucket, as well as any number
of arguments to the application, spark-submit, as well as your action on
failure.</p>
<p>The setup will take some time to finish, so in the meantime you should
configure a proxy for the web interfaces. More detailed information can be
found on the <a href="http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html">AWS website</a>. You can check the logs in your S3 bucket, or the
web interfaces to track the progress of your application and whether any errors
have occurred.</p>
<p>By forwarding the web interfaces you will also have access to Apache Ganglia.
Ganglia is a tool that allows you to monitor your cluster for incoming and
outgoing network, CPU load, memory pressure, and other useful metrics. They can
help to characterize the workload at hand, and help optimizing computation
times. An example of its interface is shown in the figure below.</p>
<p><img src="getting-started/../assets/images/ganglia.png" alt="Ganglia screenshot" /></p>
<p>It's not uncommon to run into problems when you first deploy your application
to AWS, here are some general clues:</p>
<ul>
<li>
<p>You can access S3 files directly using Spark, so via
<code>SparkContext.textFile</code> and <code>SparkSession.read.csv</code>, but not using the OS,
so using an ordinary <code>File</code> java class will not work. If you want to load a
file to the environment, you will have to figure out a workaround.</p>
</li>
<li>
<p>You can monitor the (log) output of your master and worker nodes in Yarn,
which you can access in the web interfaces. It might help you to insert
some helpful logging messages in your Application.</p>
</li>
<li>
<p>Scale your application by increasing the workload by an order of magnitude
at a time, some bugs only become apparent when you have a sufficient load
on your cluster and a sufficient cluster size. In terms of cost, it's also
much cheaper if you do your debugging incrementally on smaller clusters.</p>
</li>
<li>
<p>Ensure that your cluster is running in actual cluster mode (can be visually
confirmed by checking the load on the non-master nodes in Ganglia).</p>
</li>
</ul>
<h2><a class="header" href="#apache-kafka-1" id="apache-kafka-1">Apache Kafka</a></h2>
<p>Apache Kafka is a distributed streaming platform. The core abstraction is that
of a message queue, to which you can both publish and subscribe to streams of
records. Each queue is named by means of a topic. Apache Kafka is:</p>
<ul>
<li>Resilient by means of replication;</li>
<li>Scalable on a cluster;</li>
<li>High-throughput and low-latency; and</li>
<li>A persistent store.</li>
</ul>
<p>Kafka consists of 4 APIs, from the Kafka docs:</p>
<h3><a class="header" href="#the-producer-api" id="the-producer-api">The Producer API</a></h3>
<p>Allows an application to publish a stream of records to one or more Kafka
topics.</p>
<h3><a class="header" href="#the-consumer-api" id="the-consumer-api">The Consumer API</a></h3>
<p>Allows an application to subscribe to one or more topics and process the
stream of records produced to them.</p>
<h3><a class="header" href="#the-streams-api" id="the-streams-api">The Streams API</a></h3>
<p>Allows an application to act as a stream processor, consuming an input
stream from one or more topics and producing an output stream to one or
more output topics, effectively transforming the input streams to output
streams.</p>
<h3><a class="header" href="#the-connector-api" id="the-connector-api">The Connector API</a></h3>
<p>Allows building and running reusable producers or consumers that connect
Kafka topics to existing applications or data systems. For example, a
connector to a relational database might capture every change to a table.</p>
<p>Before you start with the lab, please read the <a href="https://kafka.apache.org/intro">Introduction to Kafka on the Kafka
website</a>, to become familiar with the Apache
Kafka abstraction and internals. A good introduction to the <a href="https://docs.confluent.io/current/streams/quickstart.html">Kafka stream API can be found
here</a>. We recommend
you go through the code and examples.</p>
<p>We will again be using Scala for this assignment. Although Kafka's API is
completely written in Java, the streams API has been wrapped in a Scala API for
convenience. You can find the Scala KStreams documentation
<a href="https://developer.lightbend.com/docs/api/kafka-streams-scala/0.2.1/com/lightbend/kafka/scala/streams/KStreamS.html">here</a>,
for API docs on the different parts of Kafka, like <code>StateStores</code>, please refer
to <a href="https://kafka.apache.org/23/javadoc/overview-summary.html">this link</a>.</p>
<h2><a class="header" href="#openstreetmap-1" id="openstreetmap-1">OpenStreetMap</a></h2>
<p><a href="getting-started/(https://www.openstreetmap.org)">OpenStreetMap</a> is a project where you can download free geographic data from
the whole world. Such data will be used as the only data source for our queries
in this lab. The project has an excellent <a href="https://wiki.openstreetmap.org/wiki/Main_Page">Wiki</a> where a lot of information
about the structure of the dataset may be found.</p>
<p>OpenStreetMap data knows three map elements:</p>
<ul>
<li><a href="https://wiki.openstreetmap.org/wiki/Node">Nodes</a> : a single point on the map, e.g. to mark the location of a <a href="https://www.openstreetmap.org/node/4829046021">brewery</a>.</li>
<li><a href="https://wiki.openstreetmap.org/wiki/Way">Ways</a> : an ordered lists of nodes, e.g. to represent (a part of) a <a href="https://www.openstreetmap.org/way/7624546">street</a>.</li>
<li><a href="https://wiki.openstreetmap.org/wiki/Relation">Relations</a> : a group of other elements, e.g. to represent the <a href="https://www.openstreetmap.org/relation/47798">boundary</a> and
center of a city.</li>
</ul>
<p>All map elements may have associated <a href="https://wiki.openstreetmap.org/wiki/Tags">tags</a> that expose more information about
what they represent. For example, we may look at <a href="https://www.openstreetmap.org/node/3376839743">some node</a>, and find that it
represents the Delft central station, with the following tags:</p>
<table><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody>
<tr><td>name</td><td>Delft</td></tr>
<tr><td>public_transport</td><td>station</td></tr>
<tr><td>railway</td><td>station</td></tr>
<tr><td>railway:ref</td><td>Dt</td></tr>
<tr><td>train</td><td>yes</td></tr>
<tr><td>wheelchair</td><td>yes</td></tr>
<tr><td>wikidata</td><td>Q800653</td></tr>
<tr><td>wikipedia</td><td>nl:Station Delft</td></tr>
</tbody></table>
<p>Many tags have an explanation on the Wiki, for example the <a href="https://wiki.openstreetmap.org/wiki/Key:wheelchair">wheelchair</a> tag,
where its value describes an indication of the level of accessibility for people
in wheelchairs to, in this case, the station.</p>
<p>Feel free to browse around the wiki to discover other tags. We can spoil that
<a href="https://wiki.openstreetmap.org/wiki/Brewery">this</a> page contains some useful tags that you may want to use for your lab
assignments.</p>
<h3><a class="header" href="#preparing-the-dataset-for-lab-1" id="preparing-the-dataset-for-lab-1">Preparing the dataset for Lab 1</a></h3>
<p>We will now explain how to prepare our dataset for lab 1. Because the query that
we are interested in is too large to process during our initial short
development iterations of implementing and debugging, its useful to start off
with a small subset of the data (in lab 1), until your implementation is stable
enough to be moved to a large cluster (in lab 2), to process the whole thing!</p>
<p>Furthermore, to be able to leverage SparkSQL, we must convert the OpenStreetMap
data to a tabular format. We will use the ORC format for this. Follow the steps
below to end up with an ORC file for lab 1.</p>
<p>Luckily, some people have already chopped up the whole world into manageable
pieces. In our case, we will start off with just the province of Zuid-Holland,
where Delft is located. Also, some people have written <a href="https://github.com/mojodna/osm2orc">a conversion tool</a> for
us already, that helps us convert the <code>.osm.pbf</code> file into an <code>.orc</code> file.</p>
<ul>
<li>
<p>Download the <a href="https://download.geofabrik.de/europe/netherlands.html">province of Zuid-Holland</a>. You need to get the <code>.osm.pbf</code> file.</p>
</li>
<li>
<p>Download and extract v.0.5.5 of <a href="https://github.com/mojodna/osm2orc/releases/download/v0.5.5/osm2orc-0.5.5.tar.gz">osm2orc</a> (click the link to download
immediately).</p>
</li>
<li>
<p>Run osm2orc to obtain the ORC file of Zuid-Holland. osm2orc can be found in
the <code>bin/</code> folder of the tool. Example usage in Linux:</p>
</li>
</ul>
<pre><code class="language-console">./osm2orc /path/to/zuid-holland-latest.osm.pbf /destination/for/zuid-holland.orc
</code></pre>
<p>You can also use the Dockerfile provided in the lab 1 repository to build an image
that includes this tool:</p>
<ul>
<li>First build the <code>osm2orc</code> image (from the root of the lab 1 repository):</li>
</ul>
<pre><code class="language-bash">docker build --target osm2orc -t osm2orc .
</code></pre>
<ul>
<li>Run the conversion tool in a container. Make sure the source <code>.osm.pbf</code> file is
downloaded in your current working directory.</li>
</ul>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io osm2orc /io/zuid-holland-latest.osm.pbf /io/zuid-holland.orc
</code></pre>
<p>You will now have the <code>zuid-holland.orc</code> file somewhere on your machine. We will
use this file as our input. Make sure you understand <a href="http://spark.apache.org/docs/2.4.6/sql-data-sources-load-save-functions.html">how to load the file into
Spark</a>.</p>
<h3><a class="header" href="#openstreetmap-schema" id="openstreetmap-schema">OpenStreetMap schema</a></h3>
<p>Once you have loaded the data set, it's possible to print out the schema of the
data, for example, on the Spark shell. Note that this assumes you have loaded
the data set as a Spark DataFrame called <code>df</code>.</p>
<pre><code>scala&gt; df.printSchema()
root
 |-- id: long (nullable = true)
 |-- type: string (nullable = true)
 |-- tags: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)
 |-- lat: decimal(9,7) (nullable = true)
 |-- lon: decimal(10,7) (nullable = true)
 |-- nds: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- ref: long (nullable = true)
 |-- members: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- type: string (nullable = true)
 |    |    |-- ref: long (nullable = true)
 |    |    |-- role: string (nullable = true)
 |-- changeset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- uid: long (nullable = true)
 |-- user: string (nullable = true)
 |-- version: long (nullable = true)
 |-- visible: boolean (nullable = true)
</code></pre>
<p>And we may look at a tiny part of the data, say the first three rows:</p>
<pre><code>scala&gt; df.show(3)
+-------+----+----+----------+---------+---+-------+---------+-------------------+---+----+-------+-------+
|     id|type|tags|       lat|      lon|nds|members|changeset|          timestamp|uid|user|version|visible|
+-------+----+----+----------+---------+---+-------+---------+-------------------+---+----+-------+-------+
|2383199|node|  []|52.0972000|4.2997000| []|     []|        0|2011-10-31 12:45:53|  0|    |      3|   true|
|2383215|node|  []|52.0990543|4.3002070| []|     []|        0|2020-01-08 20:50:29|  0|    |      6|   true|
|2716646|node|  []|52.0942524|4.3354580| []|     []|        0|2011-12-26 23:12:28|  0|    |      3|   true|
+-------+----+----+----------+---------+---+-------+---------+-------------------+---+----+-------+-------+
</code></pre>
<p>Here we can observe that the first three rows are elements of the <code>&quot;node&quot;</code> type.
Note that there are two other types of elements, as explained previously. We
also have the <code>&quot;way&quot;</code> and <code>&quot;relation&quot;</code>. There are all flattened into this single
table by the conversion tool.</p>
<p>How big is this dataframe?</p>
<pre><code>scala&gt; df.count()
res1: Long = 18426861
</code></pre>
<p>It has over 18 million rows already, and this is just the province of
Zuid-Holland!</p>
<h1><a class="header" href="#lab-1" id="lab-1">Lab 1</a></h1>
<p>In this lab, we will design and develop an application to process <a href="https://www.openstreetmap.org">OpenStreetMap</a>
data. For a small subset of the planet (the Zuid-Holland provice of the
Netherlands), the application should output a list of cities and <a href="https://wiki.openstreetmap.org/wiki/Brewery">brewery</a>
counts, ordered by the number of breweries.</p>
<p>You will run this application on a small subset of data on your local computer.
You will use this to:</p>
<ol>
<li>get familiar with the Spark APIs,</li>
<li>analyze the application's scaling behavior, and</li>
<li>draw some conclusions on how to run it efficiently in the cloud.</li>
</ol>
<p>It is up to you how you want to define <em>efficiently</em>, which can be in terms of
performance, cost, or a combination of the two.</p>
<p>You may have noticed that the first lab does not contain any supercomputing,
let alone big data. For lab 2, you will deploy your code on AWS, in an actual
big data cluster, in an effort to scale up your application to process a larger
dataset. It is up to you to find the configuration that will get you the most
efficiency, as per your definition.</p>
<h2><a class="header" href="#before-you-start" id="before-you-start">Before you start</a></h2>
<p>We recommend you read all the relevant sections on Scala and Spark in the
guide. Make sure you have Docker (or the required tools) up-and-running and that
you have built the required images for Spark and SBT, as per the instructions.
You can verify your set-up by going through the steps of the Spark tutorial.</p>
<p>Download the template project from <a href="https://github.com/abs-tudelft/sbd">lab's GitHub repository</a> (contained in
the <code>templates/lab1/</code> folder), either by forking or cloning the repository, or
downloading a zip file. You should execute all Docker commands from this
project folder. Rename the Scala file and the class it contains to something
meaningful. Update the project name in <code>build.sbt</code>. The project folder also
contains a <code>data/</code> directory, which will contain the data you download for
testing, as explained in the following. The <code>data/</code> folder is automatically
mounted in the working directory of relevant containers.</p>
<h2><a class="header" href="#assignment" id="assignment">Assignment</a></h2>
<p>Your task is to write a Spark application that reads the OpenStreetMap <code>.orc</code>
dataset the and outputs a sorted <code>.orc</code> file with the following schema:</p>
<pre><code class="language-scala">import org.apache.spark.sql.types._

val schema = StructType(
      Array(
        StructField(&quot;city&quot;, StringType),
        StructField(&quot;breweries&quot;, IntegerType)
      )
    )
</code></pre>
<p>where <code>city</code> is the name of a city, and <code>breweries</code> is the number of breweries
in this city. For your report please mention the top 10 cities (in terms of
brewery count).</p>
<p>We will use the <a href="https://download.geofabrik.de/europe/netherlands/zuid-holland-latest.osm.pbf">Zuid-Holland</a>
subset provided by <a href="lab1/geofabrik.de/">Geofabrik</a>. The <code>.osm.pbf</code> files can be
converted to <code>.orc</code> files using the tool mentioned in the introduction.</p>
<p>Please note that using other datasets for this assignment is <strong>not</strong> allowed.</p>
<p>Try to keep the following in mind when building your application:</p>
<ul>
<li>Functionality: providing a correct answer to this query is not trivial, attempt
to make your application as robust as possible.</li>
<li>Scalable: your application should keep functioning correctly when the size of
the input data set grows larger.</li>
</ul>
<h2><a class="header" href="#deliverables" id="deliverables">Deliverables</a></h2>
<p>The deliverables for this lab are:</p>
<ol>
<li>Spark application</li>
<li>Report</li>
</ol>
<p>Deliverables should be contained in your project repository.</p>
<h3><a class="header" href="#spark-application" id="spark-application">Spark Application</a></h3>
<p>All Scala source files required to build your Spark application should be
committed to your repository.</p>
<h3><a class="header" href="#report" id="report">Report</a></h3>
<p>The results and analysis in the <code>README.md</code> markdown file in the root of your
repository.</p>
<h1><a class="header" href="#rubric-for-lab-1" id="rubric-for-lab-1">Rubric for Lab 1</a></h1>
<h2><a class="header" href="#course-learning-objectives" id="course-learning-objectives">Course Learning Objectives</a></h2>
<p>For your convenience, we repeat the course learning objectives.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<h2><a class="header" href="#criteria" id="criteria">Criteria</a></h2>
<p>Lab 1 is graded by five criteria:</p>
<ul>
<li><a href="lab1/rubric.html#functionality">Functionality</a></li>
<li><a href="lab1/rubric.html#scalability">Scalability</a></li>
<li><a href="lab1/rubric.html#libraries">Libraries</a></li>
<li><a href="lab1/rubric.html#measurements">Measurements</a></li>
<li><a href="lab1/rubric.html#analysis">Analysis</a></li>
</ul>
<p>We list indicators for specific grades below. Please note these are indicators
only. Under specific unexpected circumstances, TA's may use other indicators,
that are not written down here, to modify the grade. This means that these
indicators should not be used as an exhaustive check-list for the grade, but do
provide a strong recommendation for a specific grade.</p>
<h3><a class="header" href="#functionality" id="functionality">Functionality</a></h3>
<ul>
<li>Weight: (40%)</li>
<li>Related Learning Objectives: L1, L2, L3</li>
</ul>
<p>The program functions correctly, delivering an answer to the query described
in the description.</p>
<p>Since the dataset is maintained by volunteers, there may be missing and/or 
inconsistent pieces of information.</p>
<p>Potential solutions know several robustness levels:</p>
<ol>
<li>
<p>Using only the directly available information, without augmenting the data 
with e.g. geospatial calculations, resulting in answers that may not include
data points that could have been augmented at higher robustness levels.</p>
</li>
<li>
<p>Using indirectly related data to augment the data through simple (e.g. 
geospatial) calculations, resulting in an approximate answer.</p>
</li>
<li>
<p>Using indirectly related data to augment the data through geospatial 
calculations that produce an as-correct-as-possible answer.</p>
</li>
</ol>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The program does not compile.</td></tr>
<tr><td></td><td>The program exits with errors not due to the user.</td></tr>
<tr><td></td><td>The program does not produce the correct output compatible with any of the robustness levels.</td></tr>
<tr><td>6 (adequate)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors, unless the user does something wrong.</td></tr>
<tr><td></td><td>The robustness level of the program is 1.</td></tr>
<tr><td>8 (good)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The robustness level of the program is 2.</td></tr>
<tr><td>10 (excellent)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The robustness level of the program is 3.</td></tr>
</tbody></table>
<h3><a class="header" href="#scalability" id="scalability">Scalability</a></h3>
<ul>
<li>Weight: (20%)</li>
<li>Related Learning Objectives: L1, L2</li>
</ul>
<p>The program is constructed in such a way that in the case multiple computational
nodes (e.g. more Spark workers) work on the problem concurrently, there is 
potential for the performance of the program to increase.</p>
<p>An example when this is not the case is where a <code>map()</code> is not applied to a 
distributed dataset (e.g. an <code>RDD</code> or a <code>DataSet</code>), but perhaps by mistake
the student has first used <code>dataset.collect()</code> (taking the data out of the Spark 
context) follow by a plain Scala <code>.map()</code>. The map is now applied on a locally
collected dataset, preventing other nodes from performing useful work.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>More than one step is implemented in a non-scalable fashion, where it could have been implemented in a scalable fashion.</td></tr>
<tr><td>6 (adequate)</td><td>There is at most one step implemented in a non-scalable fashion, where it could have been implemented in a scalable fashion.</td></tr>
<tr><td>8 (good)</td><td>All steps are implemented in a scalable fashion where applicable.</td></tr>
<tr><td>10 (excellent)</td><td>As with (good), in addition to comments in the code describing for each step that causes a shuffle, that it does so and why.</td></tr>
</tbody></table>
<h3><a class="header" href="#libraries" id="libraries">Libraries</a></h3>
<ul>
<li>Weight: (10%)</li>
<li>Related Learning objectives: L1, L3</li>
</ul>
<p>The program is constructed by using Spark SQL.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student has not used Spark SQL to implement the specified functionality.</td></tr>
<tr><td>6 (adequate)</td><td>The student has used Spark SQL to implement the specified functionality.</td></tr>
<tr><td>8 (good)</td><td>The student has used Spark SQL and does not construct compute paths by combining more primitive Spark functions into functions that already exist in the Spark (SQL) API.</td></tr>
<tr><td>10 (excellent)</td><td>The student has introduced abstractions for re-usability. E.g. they have developed a library on top of the Spark libraries providing an easier-to-use API to make similar queries.</td></tr>
</tbody></table>
<h3><a class="header" href="#measurements" id="measurements">Measurements</a></h3>
<ul>
<li>Weight: (5%)</li>
<li>Related Learning Objectives: L5</li>
</ul>
<p>The run-time of various computational steps of the program are profiled (this 
functionality exists in Spark). The run-time of the specified computational
steps is reported. </p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student has not reported any measurements.</td></tr>
<tr><td>10 (excellent)</td><td>The student has reported the run-time of the specified computational steps exposed by Spark's profiling functionality.</td></tr>
</tbody></table>
<h3><a class="header" href="#analysis" id="analysis">Analysis</a></h3>
<ul>
<li>Weight: (25%)</li>
<li>Related Learning Objectives: L1, L2, L3, L5</li>
</ul>
<p>The student shows a thorough understanding of the produced code and its behavior
in the context of Apache Spark. This is conveyed through the code, comments in
the code, and the report.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student shows a lack of understanding of the constructs used in the solution.</td></tr>
<tr><td></td><td>The code does not contain any descriptive comments.</td></tr>
<tr><td></td><td>There report provided cannot be considered a reasonable attempt.</td></tr>
<tr><td>6 (adequate)</td><td>The student shows a decent understanding of the constructs used in their solution, but often makes minor mistakes.</td></tr>
<tr><td></td><td>The student has explained most non-trivial constructs in their solution.</td></tr>
<tr><td></td><td>The student explains the measurement results, but gives no hypotheses for anomalies.</td></tr>
<tr><td>8 (good)</td><td>The student shows a decent understanding of the constructs used in their solution, and only makes minor to negligible mistakes in their analysis.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
<tr><td></td><td>The student explains all aspects of the measurement results, and gives acceptable hypotheses for anomalies.</td></tr>
<tr><td>10 (excellent)</td><td>The student shows a thorough understanding of the constructs used in their solution, without making any mistakes.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
<tr><td></td><td>The student explains all aspects of the measurement results, and gives a correct thorough explanation for anomalies.</td></tr>
</tbody></table>
<h1><a class="header" href="#lab-2" id="lab-2">Lab 2</a></h1>
<p>This lab assignment is about large-scala data processing using Spark and Amazon
EMR.</p>
<h2><a class="header" href="#before-you-start-1" id="before-you-start-1">Before you start</a></h2>
<p>To make sure this assignment is challenging and instructive for all students, we
require all Spark applications to have a level 3 robustness implementation. Please
use the guide below to upgrade your application if needed.</p>
<p><strong>We assume everybody has access to AWS credits via both the GitHub developer
pack and the AWS classroom in their lab group! If this is not the case, please
ask for help, or send us an email.</strong></p>
<p><strong>You pay for cluster per commissioned minute. After you are done working with a
cluster, please terminate the cluster, to avoid unnecessary costs.</strong></p>
<p>Make sure you have read the introduction on Amazon Web services in the guide
chapter before starting the assignment.</p>
<h3><a class="header" href="#level-3-robustness-guide" id="level-3-robustness-guide">Level 3 robustness guide</a></h3>
<p>The description here describes how you can upgrade your application to a level 3 robustness implementation (as listed in the rubric of lab 1).</p>
<p>Please be aware that:</p>
<ul>
<li>If the method in your application from lab 1 is the same as the one described below, but implemented differently, you are <strong>not required</strong> to modify it.</li>
<li>If the method in your application from lab 1 is more robust than the one described below, you are <strong>allowed to modify your application to be less robust</strong> if you have scalability or performance issues.</li>
</ul>
<p>This guide uses relations to construct polygons for city boundaries and then performs <a href="https://en.wikipedia.org/wiki/Point_in_polygon">point-in-polygon</a> tests to check if a brewery is within a certain city.</p>
<ol>
<li>Read input data set and split into:</li>
</ol>
<ul>
<li><a href="https://wiki.openstreetmap.org/wiki/Node"><code>nodes</code></a></li>
<li><a href="https://wiki.openstreetmap.org/wiki/Way"><code>ways</code></a></li>
<li><a href="https://wiki.openstreetmap.org/wiki/Relation"><code>relations</code></a></li>
</ul>
<p>Make sure you understand the difference between these types and how to dereference IDs in both ways and relations:</p>
<ul>
<li>Ways reference nodes in the <code>nds</code> field. Typically you <a href="https://spark.apache.org/docs/2.4.6/api/scala/index.html#org.apache.spark.sql.functions$@posexplode(e:org.apache.spark.sql.Column):org.apache.spark.sql.Column"><code>posexplode</code></a> the field with references and then <code>join</code> with the <code>nodes</code> on ID. Use <code>posexplode</code> instead of <code>explode</code> because nodes in a way are ordered, and this method allows you to recover that information.</li>
</ul>
<p>Below is an incomplete example that shows how you could dereference nodes for ways using the Dataframe API. This may look a bit different if you are using the Dataset API.</p>
<pre><code class="language-scala">val nodes = df
      .filter('type === &quot;node&quot;)
      .select(
        'id.as(&quot;node_id&quot;),
        ...
      )

val ways = df
      .filter('type === &quot;way&quot;)
      .select(
        'id.as(&quot;way_id&quot;),
        ...,
        posexplode($&quot;nds.ref&quot;)
      )
      .select(
        'way_id,
        ...,
        struct('pos, 'col.as(&quot;id&quot;)).as(&quot;node&quot;)
      )
      .join(nodes, $&quot;node.id&quot; === 'node_id)
      .groupBy('way_id)
      .agg(
        array_sort(
          collect_list(
            struct($&quot;node.pos&quot;.as(&quot;pos&quot;), ...)
          )
        ).as(&quot;way&quot;)
      )
</code></pre>
<ul>
<li>Relations reference nodes and ways in the <code>members.ref</code> field. You can <code>posexplode</code> the members field and join with <code>ways</code> and <code>nodes</code>. You need to <code>posexplode</code> and <code>join</code> the <code>ways</code> again to dereference the nodes that belong to that way. Make sure you understand the data in the <code>members.role</code> and <code>members.type</code> fields.</li>
</ul>
<p>To collect the exploded and joined information in single rows you can <code>groupBy</code> and <code>agg</code> with <code>collect_list</code>. For ways you <code>groupBy</code> the ID of the way and collect the list of nodes (maintain the order). For relations you first <code>groupBy</code> relation ID and way ID to collect the list of nodes belonging to the ways. Then you can <code>groupBy</code> relation ID to collect the ways in the relation.</p>
<ol start="2">
<li>Find breweries</li>
</ol>
<p>Use tag information from the <a href="https://wiki.openstreetmap.org/wiki/Brewery">Brewery wiki</a> to find coordinates of breweries. Nodes have coordinate information, however ways and relations require dereferencing IDs to get the nodes that make up the way or relation. For breweries you can select any node (coordinate) or average of the coordinates of the nodes that make up the way or relation. This is based on the assumption that breweries (ways and relations) don't cross city boundaries. A more robust implementation would be to recover the geometry and perform contains checks later, but this is not a requirement.</p>
<ol start="3">
<li>Find cities</li>
</ol>
<p>Use <a href="https://wiki.openstreetmap.org/wiki/Relation:boundary">boundary relations</a> to construct polygons for city boundaries. The use of third-party libraries for geometry types is encouraged. We recommend <a href="https://github.com/locationtech/jts">JTS</a>. You can use <a href="https://www.geomesa.org/">GeoMesa</a> <a href="https://www.geomesa.org/documentation/stable/user/spark/spark_jts.html">Spark JTS</a> for serialization support of geometry types. Hint take a look at the <a href="https://locationtech.github.io/jts/javadoc/org/locationtech/jts/operation/polygonize/Polygonizer.html"><code>Polygonizer</code></a> class.
You can also use other libraries e.g. <a href="https://github.com/apache/incubator-sedona">Apache Sedona</a>.</p>
<ol start="4">
<li>Check if a brewery is within the boundary of a city</li>
</ol>
<p>Perform the point-in-polygon check for breweries and cities. Use of the implementation provided your third-party library is strongly recommended.</p>
<p>If you get stuck please contact one of the TAs.</p>
<h2><a class="header" href="#assignment-1" id="assignment-1">Assignment</a></h2>
<p>Your task is to run your Spark application on the entire <a href="https://registry.opendata.aws/osm/">OpenStreetMap data
set</a>. The <a href="https://open.quiltdata.com/b/osm-pds">entire
planet</a> is available on Amazon S3.
Processing the entire planet requires a significant amount of resources, which
is why an iterative process of improving your application and running it on
increasing input data set sizes is required.</p>
<p>Start by downloading <a href="https://download.geofabrik.de/europe/netherlands-latest.osm.pbf">the
Netherlands</a>
and converting it to an ORC file locally. Before running your application on
Amazon EMR you should run your application locally on the Netherlands. <strong>This
helps you catch performance and scalability issues early-on and prevents you
from wasting your AWS credits</strong>.</p>
<p>Use the Spark <a href="https://spark.apache.org/docs/2.4.7/tuning.html">tuning guide</a>
and the <a href="https://spark.apache.org/docs/2.4.7/sql-performance-tuning.html">SQL performance tuning
page</a> to
understand how you can improve the scalability and performance of your
application.</p>
<p>Understanding how Spark executes your application is the first step towards
optimizing it. Use the
<a href="https://spark.apache.org/docs/2.4.7/api/scala/index.html#org.apache.spark.sql.Dataset@explain():Unit"><code>.explain</code></a>
method on your resulting dataset and check the <a href="https://spark.apache.org/docs/latest/web-ui.html#sql-tab">SQL
tab</a> in your Spark
History Server.</p>
<h3><a class="header" href="#running-your-application-on-amazon-emr" id="running-your-application-on-amazon-emr">Running your application on Amazon EMR</a></h3>
<p>To run Spark on multiple nodes we are using EMR for this assignment. Selecting
suitable instance types and cluster configurations to efficiently map resource
requirements from your application is an important part of this assignment. Next
to modifying your application, understanding resource utilization and using
different cluster configurations are part of the iterative process to
efficiently scale your application.</p>
<p>When you feel confident your application is ready to process a bigger input data
set (the Netherlands should run in minutes on a modern laptop), you can package
your application for execution on EMR. The
<a href="https://github.com/sbt/sbt-assembly">sbt-assembly</a> plugin (to build fat JARs -
that include dependencies you can use to optimize your implementation) is
available in your project so run the <code>assembly</code> command to package your
application.</p>
<p>Please note:</p>
<ul>
<li>
<p>When using Amazon's services, please <strong>use the N.Virgina region (<code>us-east-1</code>).
This is where the S3 buckets with the data sets are hosted.</strong> Create your
buckets and clusters in this region. This is the only region that should be
used during this course.</p>
</li>
<li>
<p>Always start with a small number of small instance types e.g. 1 master node
(<code>c5.xlarge</code>) and 5 core nodes (<code>c5.xlarge</code>). Make sure your application is
scalable before spinning up large clusters (and/or more expensive instance
types) to prevent wasting credits.</p>
</li>
<li>
<p>Check the following links for information about configuring Spark on EMR:</p>
<ul>
<li>https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html</li>
<li>https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/</li>
</ul>
</li>
<li>
<p>You don't have to specify a master in your Spark Session builder or in the
arguments of spark-submit.</p>
</li>
<li>
<p>Write the resulting ORC file to your S3 bucket.
(<code>s3://&lt;your-bucket&gt;/output.orc</code>)</p>
</li>
<li>
<p>Scalability comes at a cost, you can't ignore a proper trade-off between
runtime and cost. For example,decreasing the run time by 10% while increasing
the monetary cost by 500% is typically not acceptable.</p>
</li>
<li>
<p>Consider using a <a href="https://en.wikipedia.org/wiki/Spatial_database#Spatial_index">Spatial index</a> to optimize the spatial query. Use scalable third-party libraries that are suitable to be used in Spark.</p>
</li>
</ul>
<h3><a class="header" href="#data-sets" id="data-sets">Data sets</a></h3>
<p>The following data sets of increasing size are available on S3 and can be used
in your iterative development process:</p>
<ol>
<li>Netherlands (1.2 GB) - <code>s3://abs-tudelft-sbd20/netherlands.orc</code></li>
<li>United States (8.8 GB) - <code>s3://abs-tudelft-sbd20/us.orc</code></li>
<li>Europe (27.7 GB) - <code>s3://abs-tudelft-sbd20/europe.orc</code></li>
<li>Planet (75.8 GB) - <code>s3://osm-pds/planet/planet-latest.orc</code></li>
</ol>
<h3><a class="header" href="#runtime-indications" id="runtime-indications">Runtime indications</a></h3>
<p>The following runtime indications are available to prevent you from wasting your
credits. Make sure to experiment with other instance types and cluster
configurations as well.</p>
<p>These results are from an implementation built by the TAs. Using 1 master node
(<code>c5.xlarge</code>) and 5 core nodes (<code>c5.xlarge</code>):</p>
<ol>
<li>Netherlands - 1.5 min - aim for less than 5 minutes</li>
<li>United States - 6.9 min - aim for less than 15 minutes</li>
<li>Europe - 22 min - aim for less than 1 hour</li>
<li>Planet - 56 min - aim for less than 2 hours</li>
</ol>
<h2><a class="header" href="#deliverables-1" id="deliverables-1">Deliverables</a></h2>
<p>The deliverable for this lab are a report documenting:</p>
<ul>
<li>The iterative decision-making process</li>
<li>Improvements made to the application and their effects</li>
</ul>
<p>Add the source of your (optimized) application in the repository. You can copy
the sources from your lab 1 repository as a starting point.</p>
<p>Write the report in the <code>README.md</code> markdown file in the root of your
repository. Use the template as a starting point for your report.</p>
<h1><a class="header" href="#rubric-for-lab-2" id="rubric-for-lab-2">Rubric for Lab 2</a></h1>
<h2><a class="header" href="#course-learning-objectives-1" id="course-learning-objectives-1">Course Learning Objectives</a></h2>
<p>For your convenience, we repeat the course learning objectives.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<h2><a class="header" href="#criteria-1" id="criteria-1">Criteria</a></h2>
<p>Lab 2 is graded by four criteria:</p>
<ul>
<li><a href="lab2/rubric.html#functionality">Functionality</a></li>
<li><a href="lab2/rubric.html#approach">Approach</a></li>
<li><a href="lab2/rubric.html#application">Application</a></li>
<li><a href="lab2/rubric.html#cluster">Cluster</a></li>
</ul>
<p>The grade for this lab is expressed as:</p>
<pre><code>Grade = Functionality * (Approach + Application + Cluster)
</code></pre>
<p>We list indicators for specific grades below. Please note these are indicators
only. Under specific unexpected circumstances, TA's may use other indicators,
that are not written down here, to modify the grade. This means that these
indicators should not be used as an exhaustive check-list for the grade, but do
provide a strong recommendation for a specific grade.</p>
<h3><a class="header" href="#functionality-1" id="functionality-1">Functionality</a></h3>
<ul>
<li>Weight: 100% (multiplicative)</li>
<li>Related Learning Objectives: L1, L2, L3</li>
</ul>
<p>The program adheres to at least robustness level three.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The program does not compile.</td></tr>
<tr><td></td><td>The program exits with errors not due to the user.</td></tr>
<tr><td></td><td>The program does not adhere to robustness level three.</td></tr>
<tr><td>1 (pass)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors, unless the user does something wrong.</td></tr>
<tr><td></td><td>The program adheres to robustness level three.</td></tr>
</tbody></table>
<h3><a class="header" href="#approach" id="approach">Approach</a></h3>
<ul>
<li>Weight: 50% (additive)</li>
<li>Related Learning Objectives: L1</li>
</ul>
<p>Because the development iterations of big data applications can be large in both
cost and time, the student makes careful, well-informed decisions before
executing the next iteration, and documents their decision-making approach.</p>
<p>Significant iterations that are to be documented include:</p>
<ol>
<li>The first run on the first data set after achieving robustness level 3.</li>
<li>After significant optimizations are performed and a full run is completed on 
any of the data sets.</li>
<li>When going from a smaller dataset to a larger data set results in
significant changes in where bottlenecks are in the application.</li>
</ol>
<p>Examples:</p>
<ul>
<li>As described in the previous iteration, we discovered a new bottleneck X. This
was mitigated. After re-running the application, a next bottleneck occurs in 
Y. We have thought of method A, B and C to mitigate this, and have ultimately 
chosen B because of reason Z.</li>
<li>The query was initially performed on the &quot;The Netherlands&quot; data set, and was 
now run on the USA data set. The USA has significantly more craft breweries 
per city, so bottleneck X caused by operation Y was relatively increased. We 
have therefore thought of method A, B and C to mitigate this, and have 
ultimately chosen B because of reason Z.</li>
</ul>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student does not explain why they have taken a specific approach before executing a development iteration.</td></tr>
<tr><td></td><td>There is no explicit reasoning behind a specific approach to a next development iteration.</td></tr>
<tr><td>6 (adequate)</td><td>The student describes their reasoning behind a specific approach to a next development iteration, but their description contains minor mistakes or makes a limited amount of incorrect assumptions.</td></tr>
<tr><td>8 (good)</td><td>The student describes their reasoning behind a specific approach to a next development iteration.</td></tr>
<tr><td>10 (excellent)</td><td>The student describes their reasoning behind multiple (where applicable) alternative approaches (and their selection) to a next development iteration.</td></tr>
</tbody></table>
<h3><a class="header" href="#application" id="application">Application</a></h3>
<ul>
<li>Weight: 30% (additive)</li>
<li>Related Learning Objectives: L1, L2, L3, L5</li>
</ul>
<p>The student selects and executes an appropriate improvement at the application
implementation level to overcome bottlenecks.</p>
<p>An examples where an application implementation improvement is appropriate:</p>
<ul>
<li>A dataset is reused multiple times and fits in worker memory, so the student 
has applied <code>cache()</code>.</li>
</ul>
<p>An example where an application implementation is not appropriate:</p>
<ul>
<li>A dataset is <code>repartition()</code>ed based on some arbitrary value that only works
well in a specific case (e.g. for a specific dataset), but does not work well
for other case (e.g. another dataset). Such values should be derived
dynamically.</li>
</ul>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student applies several inappropriate strategies for application improvements.</td></tr>
<tr><td>10 (excellent)</td><td>The student applies appropriate strategies for application improvement.</td></tr>
</tbody></table>
<h3><a class="header" href="#cluster" id="cluster">Cluster</a></h3>
<ul>
<li>Weight: 20% (additive)</li>
<li>Related Learning Objectives: L1, L4</li>
</ul>
<p>The student selects and executes an appropriate strategy at the cluster level to
overcome bottlenecks in their implementation, and takes into consideration the
trade-off between cost and performance.</p>
<p>An example where this is done appropriately is: if the application throughput is
bound by network I/O, the student can choose to run the application on instances
that have more network bandwidth.</p>
<p>An example where this is not done appropriately is: the performance of the
application is bound by memory size, but to mitigate this the student moves to
instance types with GPUs to obtain more compute, but not more memory, or the
other way around.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student applies several inappropriate strategies for cluster improvements.</td></tr>
<tr><td>10 (excellent)</td><td>The student applies appropriate strategies for cluster improvement.</td></tr>
</tbody></table>
<h1><a class="header" href="#lab-3" id="lab-3">Lab 3</a></h1>
<p>In the third and final lab of SBD we will be implementing a streaming
application. Spark, that we've used in the first two labs, is quite good at
processing large batches of data.</p>
<p>However, there is another class of big data applications that gather and
transform many tiny items coming from many so called <em>producers</em>, transform the
information, and send it off to many so called <em>consumers</em>, typically with a low
latency. Such applications are called <em>streaming</em> applications.</p>
<p>One example is when you're using a navigation service when driving a car. You're
producing location information about your car, this is streamed to a service
that collects this information from many other people as well. In turn it can
detect traffic jams. It gathers, transforms, and finally outputs traffic jam
information back to your phone, all in relative real time.</p>
<p>Since Apache Spark is more geared towards doing batch processing, for this lab
we will be using <em>Apache Kafka</em>; a well-known framework used a lot to create
such streaming applications in scalable fashion.</p>
<p>In typical streaming applications, we stream data into a pipeline that processes
the data by filtering and transforming it towards a desired result. At the end,
there is usually some form of visualization. Such a pipeline, that we will use
for this lab, is depicted in the figure below:</p>
<p><img src="lab3/../assets/images/kafka_pipeline.png" alt="Streaming pipeline" /></p>
<p>Your assignment will be to use Apache Kafka to transform a stream of raw data
into something more usable for our goal. Before explaining the exact goal, we
will first briefly go over the various components of the pipeline shown in the
figure.</p>
<h3><a class="header" href="#kafka-stream" id="kafka-stream">Kafka Stream</a></h3>
<p>Kafka knows the concept of <em>streams</em> that have a specific <em>topic</em>. Producers and
consumers can subscribe to topics. Through the APIs of Kafka, we can pull and 
push data from and to such Kafka streams.</p>
<h3><a class="header" href="#producer" id="producer">Producer</a></h3>
<p>The producer, contained in the <code>Producer</code> project creates (for the purpose of
this lab) randomly generated data records and outputs them on a Kafka stream
with the topic <code>events</code>.</p>
<p>For typical applications, this is some source like IOT devices, people's phones,
social media outputs, etc. Many services have streaming APIs, like e.g.
<a href="https://developer.twitter.com/en/docs/labs/sampled-stream/api-reference/get-tweets-stream-sample">Twitter</a>,
but for this lab, we just mimic a stream producing service, since that provides
a more learning-friendly environment.</p>
<p>The Producer will be provided, you don't have to do anything on the Producer.</p>
<h3><a class="header" href="#transformer" id="transformer">Transformer</a></h3>
<p>The transformer consumes records from the Kafka stream of the topic <code>events</code>.
It produces a Kafka stream of the topic <code>updates</code>.
It therefore transforms 'events' to 'updates', where usually many events are
aggregated into some statistic, or filtered by some predicate.</p>
<p>Why this intermediate step? Why can't a consumer process the <code>events</code> itself?
For typical applications, this is usually not done by the consumer of the data
because the producers send so much data on the <code>events</code> topic, that e.g. a
simple phone or a laptop could not process it --- only a scalable application
run on a cluster could process so much information so quickly. This is the type
of framework that Kafka provides.</p>
<p>This is the component that you must implement.</p>
<h3><a class="header" href="#consumer" id="consumer">Consumer</a></h3>
<p>The consumer finally acts as a <em>sink</em>, and will process the incoming updates
from the <code>updates</code> topic. It will do some final pre-processing before sending
the updates to the visualizer.</p>
<p>The Consumer will be provided, you don't have to do anything on the Consumer.</p>
<h3><a class="header" href="#visualizer" id="visualizer">Visualizer</a></h3>
<p>The visualizer acts as a webserver, providing a webpage where you can view your
updates in a real-time fashion. Once it is up and running (see next page), you
can browse to it by using this link: <a href="http://localhost:1234">localhost:1234</a>.</p>
<h3><a class="header" href="#further-reading-on-kafka" id="further-reading-on-kafka">Further reading on Kafka</a></h3>
<ul>
<li><a href="https://kafka.apache.org/intro">Kafka Intro</a></li>
<li><a href="https://kafka.apache.org/26/documentation/streams/architecture.html">Kafka Streams Architecture</a></li>
<li><a href="https://kafka.apache.org/26/documentation/streams/developer-guide/dsl-api.html">Kafka DSL API</a></li>
<li><a href="https://kafka.apache.org/26/documentation/streams/developer-guide/processor-api.html">Kafka Processor API</a></li>
<li><a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/kstream/KStream.html">Kafka KStream API docs</a></li>
<li><a href="https://github.com/confluentinc/kafka-streams-examples">Kafka Stream Examples</a></li>
<li><a href="https://github.com/azhur/kafka-serde-scala">JSON Serde support for Kafka</a></li>
<li><a href="https://jaceklaskowski.gitbooks.io/mastering-kafka-streams/">Mastering Kafka Streams</a></li>
</ul>
<h2><a class="header" href="#before-you-start-2" id="before-you-start-2">Before you start</a></h2>
<p>For this assignment a <code>docker-compose.yml</code> file is provided to easily build and
run all required services in Docker containers.
<a href="https://docs.docker.com/compose/">Docker Compose</a> is a tool for defining and
running multi-container Docker applications. Please
<a href="https://docs.docker.com/compose/install/">install</a> the latest version for
this assignment. Also take a look at the
<a href="https://docs.docker.com/compose/compose-file/">Compose file reference</a>.</p>
<p>For this assignment the following containers are defined in the
<code>docker-compose.yml</code> file:</p>
<table><thead><tr><th>Container</th><th>Description</th></tr></thead><tbody>
<tr><td><code>zookeeper-server</code></td><td>A Zookeeper server instance. Kafka requires Zookeeper to run.</td></tr>
<tr><td><code>kafka-server</code></td><td>A single Kafka server instance.</td></tr>
<tr><td><code>producer</code></td><td>A Kafka producer running the Producer application.</td></tr>
<tr><td><code>transformer</code></td><td>A Kafka stream processor running the Transformer application.</td></tr>
<tr><td><code>consumer</code></td><td>A Kafka consumer running the Consumer application and the visualizer. The visualizer can be accessed when the service is running by navigating to <a href="http://localhost:1234">localhost:1234</a>.</td></tr>
<tr><td><code>events</code></td><td>A Kafka consumer subscribed to the <code>events</code> topic, writing records to the console. Useful for debugging.</td></tr>
<tr><td><code>updates</code></td><td>A Kafka consumer subscribed to the <code>updates</code> topic, writing records to the console. Useful for debugging.</td></tr>
</tbody></table>
<p>To start the containers, navigate to the repository root directory and run
<code>docker-compose up</code>. This will build and start all the containers defined in
the compose file. To start over, stop and remove everything with
<code>docker-compose down</code>.</p>
<p>The Transformer application, which you will develop for this assignment, is
built and run in the <code>transformer</code> service container. To start an interactive
<code>sbt</code> shell use <code>docker-compose exec transformer sbt</code> from the repository root.
From there, you can <code>compile</code> and <code>run</code> the Transformer application. Make sure
the other services are up before starting your streams application. You can stop
your application before running it again after changing the source using
<code>CTRL+C</code>.</p>
<p>You can use the following <code>docker-compose</code> commands to interact with your
running containers:</p>
<table><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody>
<tr><td><code>docker-compose up</code></td><td>Create and start containers.</td></tr>
<tr><td><code>docker-compose up -d</code></td><td>Create and start containers in the background.</td></tr>
<tr><td><code>docker-compose down</code></td><td>Stop and remove containers, networks, images, and volumes.</td></tr>
<tr><td><code>docker-compose start</code></td><td>Start services.</td></tr>
<tr><td><code>docker-compose restart</code></td><td>Restart services.</td></tr>
<tr><td><code>docker-compose stop</code></td><td>Stop services.</td></tr>
<tr><td><code>docker-compose rm</code></td><td>Remove stopped containers.</td></tr>
<tr><td><code>docker-compose logs --follow &lt;SERVICE&gt;</code></td><td>View and follow log output from containers.</td></tr>
</tbody></table>
<p>For a full list of available commands please refer to the
<a href="https://docs.docker.com/compose/reference/overview/">CLI Reference</a>.</p>
<h2><a class="header" href="#assignment-2" id="assignment-2">Assignment</a></h2>
<p>Once you have created your lab 3 repository on GitHub Classroom, your job is to
implement the Transformer component in the following file:
<code>transformer/src/main/scala/Transformer.scala</code>.</p>
<p>The Transformer application this year will also be a beer-themed assignment. The
producer produces a stream of events that represent &quot;check-ins&quot; of a specific
drink in a specific city (inspired by <a href="https://untappd.com/">Untappd</a>).</p>
<p>Each check-in is supplied as a <a href="https://www.json.org/json-en.html">JSON</a> object
on the stream. Check-ins contain the following fields:</p>
<table><thead><tr><th>Field</th><th>JSON Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>timestamp</code></td><td>number</td><td>The Unix timestamp of the check-in.</td></tr>
<tr><td><code>city_id</code></td><td>number</td><td>The ID of the city where the check-in was made.</td></tr>
<tr><td><code>city_name</code></td><td>string</td><td>The name of the city where the check-in was made.</td></tr>
<tr><td><code>style</code></td><td>string</td><td>The drink style.</td></tr>
</tbody></table>
<p>Suppose we are beer connoisseurs and we are interested to learn where a lot of
check-in activity is coming from. Usually this means there is a beer festival
that we want to visit, and perhaps it is in our city or a city close-by. We do
want to make sure that there is plenty of beer with styles to our preference. So
we can filter out or include some of the more <a href="https://en.wikipedia.org/wiki/Gueuze">adventurous
styles</a>*.</p>
<p>Your task is to filter check-ins by beer style, and count the number of
check-ins per city within a time window, and produce streaming updates for the
real-time world map of the web interface.</p>
<h3><a class="header" href="#supplying-beer-styles" id="supplying-beer-styles">Supplying beer styles</a></h3>
<p>Beer styles to include are supplied in a file called <code>beer.styles</code>.
Beer styles are separated by newline. For example:</p>
<pre><code>Amber ale
Stout
Pilsener/Pilsner/Pils
</code></pre>
<p>Beer styles in this file match exactly with the names of the producer
implementation found in: <code>producer/src/main.rs</code>.</p>
<h3><a class="header" href="#supplying-the-time-window" id="supplying-the-time-window">Supplying the time window</a></h3>
<p>The time window will be supplied on the command-line as the first argument
representing the time window size in seconds.</p>
<h3><a class="header" href="#producing-updates" id="producing-updates">Producing updates</a></h3>
<p>The updates are to be sent over a Kafka stream with a JSON string value of an
update object with the following fields and types.</p>
<table><thead><tr><th>Field</th><th>JSON Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>city_id</code></td><td>number</td><td>The ID of the city for this update.</td></tr>
<tr><td><code>count</code></td><td>number</td><td>The number of check-ins for this city within our window.</td></tr>
</tbody></table>
<p>For example, if we receive the following JSONs on the <code>events</code> stream:</p>
<pre><code class="language-json">{&quot;timestamp&quot;:1, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;style&quot;:&quot;Brown ale&quot;}
{&quot;timestamp&quot;:2, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;style&quot;:&quot;Bitter&quot;}
{&quot;timestamp&quot;:3, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;style&quot;:&quot;Weizenbock&quot;}
{&quot;timestamp&quot;:4, &quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;style&quot;:&quot;Schwarzbier&quot;}
</code></pre>
<p>And if we would set our recent window to a 3 milliseconds range, we
need to produce the following records on the <code>updates</code> stream:</p>
<pre><code class="language-C++"> K   V
&quot;1&quot;, { &quot;city_id&quot;: 1, &quot;count&quot;: 1 } // t=1 ms, update Delft with the new recent check-in
&quot;1&quot;, { &quot;city_id&quot;: 1, &quot;count&quot;: 2 }  // t=2 ms, update Delft with the new recent check-in
&quot;1&quot;, { &quot;city_id&quot;: 1, &quot;count&quot;: 3 }  // t=3 ms, update Delft with the new recent check-in
&quot;1&quot;, { &quot;city_id&quot;: 1, &quot;count&quot;: 2 }  // t=4 ms, update Delft, the oldest check-in went out of the window
&quot;2&quot;, { &quot;city_id&quot;: 2, &quot;count&quot;: 1 }  // t=4 ms, update Rotterdam to have 1 check-in recently
&quot;1&quot;, { &quot;city_id&quot;: 1, &quot;count&quot;: 1 }  // t=5 ms, update Delft to remove old check-ins
&quot;1&quot;, { &quot;city_id&quot;: 1, &quot;count&quot;: 0 }  // t=6 ms, update Delft to remove old check-ins
&quot;2&quot;, { &quot;city_id&quot;: 2, &quot;count&quot;: 0 }  // t=7 ms, update Rotterdam to remove old check-ins
</code></pre>
<p>Please note that the keys are ignored by the consumer, and set to the city IDs in this example.</p>
<h3><a class="header" href="#general-hints-and-recommended-approach" id="general-hints-and-recommended-approach">General hints and recommended approach</a></h3>
<ul>
<li>Streaming frameworks like Kafka usually work with very strong notions of
<strong>stateless</strong> and <strong>stateful</strong> operations.
<ul>
<li>An example of a <strong>stateless</strong> operation is to filter records of a stream based
on their content.</li>
<li>An example of a <strong>stateful</strong> operation is to update some existing data
structure based on streamed records.</li>
</ul>
</li>
<li>Keeping track of check-ins that have been included in your current window of
interest is <strong>stateful</strong>, and can be done in an abstraction called a <strong>state
store</strong>.
<ul>
<li>You can operate on state stores whenever a new record arrives using so called
<strong>stateful transformations</strong> of records.</li>
<li>It is recommended to use the
<a href="https://kafka.apache.org/26/documentation/streams/developer-guide/processor-api.html">Processor API</a>
for this. Check <a href="https://kafka.apache.org/26/documentation/streams/developer-guide/dsl-api.html#applying-processors-and-transformers-processor-api-integration">this documentation</a> on how to apply processors and transformers. Consider the differences between <a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/kstream/KStream.html#process-org.apache.kafka.streams.processor.ProcessorSupplier-java.lang.String...-">processors</a> and <a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/kstream/KStream.html#transform-org.apache.kafka.streams.kstream.TransformerSupplier-java.lang.String...-">transformers</a> and pick the one that best suits this application.</li>
<li>While it is technically possible to use Kafka's Windowing abstractions, it
is <strong>not recommended</strong>, because it does not exactly match our use-case.</li>
</ul>
</li>
<li>What is a little bit similar to building up DAGs in Spark is what in Kafka is
called building up the stream Topology.
<ul>
<li>This is also lazily evaluated and only starts doing its thing when you call
<code>.start()</code> on a <code>KafkaStreams</code>.</li>
<li>You can obtain a Topology description for debugging after using e.g.
<code>val topology = builder.build()</code> and then <code>println(topology.describe())</code>.</li>
<li>If you copy-paste the description
<a href="https://zz85.github.io/kafka-streams-viz/">in this tool</a>,
you can visualize it.</li>
</ul>
</li>
<li>You probably want to convert the JSONs to a Scala case class to be able to
process the data in a type-safe manner. It is recommended to use one of the
options from <a href="https://github.com/azhur/kafka-serde-scala">this repository</a>.
<code>circe</code> is a good option that worked for the TA's.</li>
</ul>
<h3><a class="header" href="#notes" id="notes">Notes</a></h3>
<p>* For this particular style, &quot;wild&quot; yeasts are used. Folklore says pigeons
sitting over the water reservoirs at breweries used to provide such yeasts
through their droppings!</p>
<h2><a class="header" href="#deliverables-2" id="deliverables-2">Deliverables</a></h2>
<p>The deliverables for this lab are:</p>
<ol>
<li>Transformer application</li>
<li>Report</li>
</ol>
<p>Deliverables should be contained in your project repository.</p>
<h3><a class="header" href="#kafka-based-application" id="kafka-based-application">Kafka-based application</a></h3>
<p>All Scala source files required to build your Kafka-based application should be
committed to your Lab 3 repository before the deadline.</p>
<h3><a class="header" href="#report-1" id="report-1">Report</a></h3>
<p>The results and analysis in the <code>README.md</code> markdown file in the root of your
repository.</p>
<h1><a class="header" href="#rubric-for-lab-3" id="rubric-for-lab-3">Rubric for Lab 3</a></h1>
<h2><a class="header" href="#course-learning-objectives-2" id="course-learning-objectives-2">Course Learning Objectives</a></h2>
<p>For your convenience, we repeat the course learning objectives.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<h2><a class="header" href="#criteria-2" id="criteria-2">Criteria</a></h2>
<p>Lab 3 is graded by the following criteria:</p>
<ul>
<li><a href="lab3/rubric.html#functionality">Functionality</a></li>
<li><a href="lab3/rubric.html#streaming">Streaming</a></li>
<li><a href="lab3/rubric.html#analysis">Analysis</a></li>
</ul>
<p>The grade for this lab is expressed as:</p>
<pre><code>Grade = Functionality + Streaming + Analysis
</code></pre>
<p>We list indicators for specific grades below. Please note these are indicators
only. Under specific unexpected circumstances, TA's may use other indicators,
that are not written down here, to modify the grade. This means that these
indicators should not be used as an exhaustive check-list for the grade, but do
provide a strong recommendation for a specific grade.</p>
<h3><a class="header" href="#functionality-2" id="functionality-2">Functionality</a></h3>
<ul>
<li>Weight: 30%</li>
<li>Related Learning Objectives: L2, L5</li>
</ul>
<p>The student has implemented an application that functions according to the 
specifications given in the lab manual.</p>
<p>With regard to the functionality of the transformer application, we stipulate
the following:</p>
<ul>
<li>For every incoming record on the <code>events</code> stream, an update is produced.</li>
<li>When event records fall outside the window of interest and cause a change in 
any stateful data structure related to the <code>updates</code> stream, an update is also
produced. Observe the example carefully.</li>
</ul>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The program does not compile.</td></tr>
<tr><td></td><td>The program exits with errors not due to the user.</td></tr>
<tr><td></td><td>The program does not produce the correct output.</td></tr>
<tr><td>6 (adequate)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors, unless the user does something wrong.</td></tr>
<tr><td></td><td>The program produces the correct updates for every event.</td></tr>
<tr><td></td><td>The program does not produce the correct updates when stateful data structures change due to events falling outside the window of interest.</td></tr>
<tr><td>10 (excellent)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The program does produces the correct output.</td></tr>
</tbody></table>
<h3><a class="header" href="#streaming" id="streaming">Streaming</a></h3>
<ul>
<li>Weight: 40%</li>
<li>Related Learning Objectives: L2, L5</li>
</ul>
<p>The student makes use of Kafka's ability to implement streamable low-latency
applications. To this end, the student avoids micro-batching as much as
possible, and applies stateless processing methods as much as possible. Of
course, there may be processing steps that do require state.</p>
<p>An example of micro-batching is as follows. Consider some input stream with some
integers: <code>{0, 1, 2, 3}</code> and an application that has to produce an output stream
with the squared value of the integers <code>{0, 1, 4, 9}</code>. (Here, assume every value
is a discrete Kafka record).  A student would be applying micro-batching when
they would place the integers in e.g. a Kafka store, and trigger the squaring
operation and producing output records e.g. periodically or e.g. when the amount
of integers reaches some threshold, processing all input records in the store at
once. The student should avoid micro-batching by using stateless
transformations, since this specific functionality does not require state.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0  (fail)</td><td>The transformer application is in more than one place implemented with stateful methods where they are not required or desired.</td></tr>
<tr><td></td><td>Micro-batching is applied in more than one place.</td></tr>
<tr><td>6  (adequate)</td><td>The transformer application is in at most one place implemented with a stateful method where it is not required or desired.</td></tr>
<tr><td></td><td>Micro-batching is applied in at most one place.</td></tr>
<tr><td>10 (excellent)</td><td>The transformer application uses stateless methods as much as possible, avoiding stateful methods where not required or desired.</td></tr>
<tr><td></td><td>The transformer application produces an update for every incoming record without applying micro-batching.</td></tr>
</tbody></table>
<h3><a class="header" href="#analysis-1" id="analysis-1">Analysis</a></h3>
<ul>
<li>Weight: (30%)</li>
<li>Related Learning Objectives: L1, L2, L3, L5</li>
</ul>
<p>The student shows a thorough understanding of the produced code and its behavior
in the context of Apache Kafka. This is conveyed through the code, comments in
the code, and the report.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student shows a lack of understanding of the constructs used in the solution.</td></tr>
<tr><td></td><td>The code does not contain any descriptive comments.</td></tr>
<tr><td></td><td>There report provided cannot be considered a reasonable attempt.</td></tr>
<tr><td>6 (adequate)</td><td>The student shows a decent understanding of the constructs used in their solution, but often makes minor mistakes.</td></tr>
<tr><td></td><td>The student has explained most non-trivial constructs in their solution.</td></tr>
<tr><td>8 (good)</td><td>The student shows a decent understanding of the constructs used in their solution, and only makes minor to negligible mistakes in their analysis.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
<tr><td>10 (excellent)</td><td>The student shows a thorough understanding of the constructs used in their solution, without making any mistakes.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
</tbody></table>
<h1><a class="header" href="#frequently-asked-questions-faq" id="frequently-asked-questions-faq">Frequently asked questions (FAQ)</a></h1>
<h2><a class="header" href="#java-11" id="java-11">Java 11</a></h2>
<p>Help! I'm getting:
<code>java.lang.IllegalArgumentException: Unsupported class file major version 55</code></p>
<p>Spark <code>2.4.6</code> only runs on Java 8.</p>
<p>If you're on Ubuntu 20.04, this version is not installed by default.
You could install it and enable it using:</p>
<pre><code>sudo apt install openjdk-8-jdk
sudo update-java-alternatives --jre-headless --jre --set java-1.8.0-openjdk-amd6
</code></pre>
<p>However, it's more convenient to <a href="guide/docker.html">use the Docker container</a> for this course.</p>
<h2><a class="header" href="#quiz-example" id="quiz-example">Quiz example</a></h2>
<p>The below questions are examples for the quiz at the end of the lab.</p>
<p>There is always only one possible answer.</p>
<ol>
<li>
<p>Which Spark script is used to launch applications on a Spark cluster?</p>
<ul>
<li>A. <code>spark_submit</code></li>
<li>B. <code>spark_drive</code></li>
<li>C. <code>spark_assemble</code></li>
<li>D. <code>spark_shell</code></li>
</ul>
</li>
<li>
<p>Given a dataframe with columns: {'a', 'b', 'c'}, which Data(frame/set) API
function can be used to create a dataframe with columns {'a', 'c'}?</p>
<ul>
<li>A. <code>select(...)</code></li>
<li>B. <code>collect(...)</code></li>
<li>C. <code>reduce(...)</code></li>
<li>D. <code>filter(...)</code></li>
</ul>
</li>
<li>
<p>The OpenStreetMap dataset contains the following three distinct elements:</p>
<ul>
<li>A. nodes, ways, relations</li>
<li>B. breweries, cities, tags</li>
<li>C. tags, nodes, cities</li>
<li>D. addr:city, admin levels, boundary</li>
</ul>
</li>
<li>
<p>The reports were implemented in a file called:</p>
<ul>
<li>A. <code>README.md</code></li>
<li>B. <code>REPORT.txt</code></li>
<li>C. <code>LAB_X.md</code></li>
<li>D. <code>build.sbt</code></li>
</ul>
</li>
<li>
<p>The artifact from SBT that is to be sent off to the Spark cluster has the
extension:</p>
<ul>
<li>A. <code>.jar</code></li>
<li>B. <code>.spark</code></li>
<li>C. <code>.sbt</code></li>
<li>D. <code>.scala</code></li>
</ul>
</li>
</ol>
<p>Answers: </p>
<ol>
<li>A</li>
<li>A</li>
<li>A</li>
<li>A</li>
<li>A</li>
</ol>
<h2><a class="header" href="#useful-links" id="useful-links">Useful links</a></h2>
<p>Below are some links that are useful:</p>
<ul>
<li><a href="https://rogerdudler.github.io/git-guide">Git cheatsheet</a></li>
</ul>
<h2><a class="header" href="#often-used-api-docs" id="often-used-api-docs">Often-used API docs:</a></h2>
<ul>
<li><a href="https://spark.apache.org/docs/2.4.6/api/scala/index.html#org.apache.spark.package">Spark all APIs</a></li>
<li><a href="https://spark.apache.org/docs/2.4.6/api/scala/index.html#org.apache.spark.sql.Dataset">Spark DataSet API</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
